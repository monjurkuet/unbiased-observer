{
  "data": {
    "edges": [
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Google Generative AI Embeddings",
            "id": "Google Generative AI Embeddings-OtCn3",
            "name": "embeddings",
            "output_types": [
              "Embeddings"
            ]
          },
          "targetHandle": {
            "fieldName": "embedding_model",
            "id": "CustomComponent-PB269",
            "inputTypes": [
              "Embeddings"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__Google Generative AI Embeddings-OtCn3{œdataTypeœ:œGoogle Generative AI Embeddingsœ,œidœ:œGoogle Generative AI Embeddings-OtCn3œ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-CustomComponent-PB269{œfieldNameœ:œembedding_modelœ,œidœ:œCustomComponent-PB269œ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "Google Generative AI Embeddings-OtCn3",
        "sourceHandle": "{œdataTypeœ:œGoogle Generative AI Embeddingsœ,œidœ:œGoogle Generative AI Embeddings-OtCn3œ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}",
        "target": "CustomComponent-PB269",
        "targetHandle": "{œfieldNameœ:œembedding_modelœ,œidœ:œCustomComponent-PB269œ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "LMStudioModel",
            "id": "LMStudioModel-bVpBg",
            "name": "model_output",
            "output_types": [
              "LanguageModel"
            ]
          },
          "targetHandle": {
            "fieldName": "agent_llm",
            "id": "Agent-s120G",
            "inputTypes": [
              "LanguageModel"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__LMStudioModel-bVpBg{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-bVpBgœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-Agent-s120G{œfieldNameœ:œagent_llmœ,œidœ:œAgent-s120Gœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "LMStudioModel-bVpBg",
        "sourceHandle": "{œdataTypeœ:œLMStudioModelœ,œidœ:œLMStudioModel-bVpBgœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
        "target": "Agent-s120G",
        "targetHandle": "{œfieldNameœ:œagent_llmœ,œidœ:œAgent-s120Gœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt Template",
            "id": "Prompt Template-qHrW7",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_prompt",
            "id": "Agent-s120G",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__Prompt Template-qHrW7{œdataTypeœ:œPrompt Templateœ,œidœ:œPrompt Template-qHrW7œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-Agent-s120G{œfieldNameœ:œsystem_promptœ,œidœ:œAgent-s120Gœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "Prompt Template-qHrW7",
        "sourceHandle": "{œdataTypeœ:œPrompt Templateœ,œidœ:œPrompt Template-qHrW7œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Agent-s120G",
        "targetHandle": "{œfieldNameœ:œsystem_promptœ,œidœ:œAgent-s120Gœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Agent",
            "id": "Agent-s120G",
            "name": "response",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-S0ZIn",
            "inputTypes": [
              "Data",
              "DataFrame",
              "Message"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__Agent-s120G{œdataTypeœ:œAgentœ,œidœ:œAgent-s120Gœ,œnameœ:œresponseœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-S0ZIn{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-S0ZInœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "Agent-s120G",
        "sourceHandle": "{œdataTypeœ:œAgentœ,œidœ:œAgent-s120Gœ,œnameœ:œresponseœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ChatOutput-S0ZIn",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-S0ZInœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "HybridVectorTool",
            "id": "CustomComponent-PB269",
            "name": "tool",
            "output_types": [
              "Tool"
            ]
          },
          "targetHandle": {
            "fieldName": "tools",
            "id": "Agent-s120G",
            "inputTypes": [
              "Tool"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__CustomComponent-PB269{œdataTypeœ:œHybridVectorToolœ,œidœ:œCustomComponent-PB269œ,œnameœ:œtoolœ,œoutput_typesœ:[œToolœ]}-Agent-s120G{œfieldNameœ:œtoolsœ,œidœ:œAgent-s120Gœ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "CustomComponent-PB269",
        "sourceHandle": "{œdataTypeœ:œHybridVectorToolœ,œidœ:œCustomComponent-PB269œ,œnameœ:œtoolœ,œoutput_typesœ:[œToolœ]}",
        "target": "Agent-s120G",
        "targetHandle": "{œfieldNameœ:œtoolsœ,œidœ:œAgent-s120Gœ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "AdvancedGraphTool",
            "id": "CustomComponent-ZCAlX",
            "name": "tool",
            "output_types": [
              "Tool"
            ]
          },
          "targetHandle": {
            "fieldName": "tools",
            "id": "Agent-s120G",
            "inputTypes": [
              "Tool"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__CustomComponent-ZCAlX{œdataTypeœ:œAdvancedGraphToolœ,œidœ:œCustomComponent-ZCAlXœ,œnameœ:œtoolœ,œoutput_typesœ:[œToolœ]}-Agent-s120G{œfieldNameœ:œtoolsœ,œidœ:œAgent-s120Gœ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "CustomComponent-ZCAlX",
        "sourceHandle": "{œdataTypeœ:œAdvancedGraphToolœ,œidœ:œCustomComponent-ZCAlXœ,œnameœ:œtoolœ,œoutput_typesœ:[œToolœ]}",
        "target": "Agent-s120G",
        "targetHandle": "{œfieldNameœ:œtoolsœ,œidœ:œAgent-s120Gœ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-19rHM",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "Agent-s120G",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__ChatInput-19rHM{œdataTypeœ:œChatInputœ,œidœ:œChatInput-19rHMœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Agent-s120G{œfieldNameœ:œinput_valueœ,œidœ:œAgent-s120Gœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "ChatInput-19rHM",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-19rHMœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Agent-s120G",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œAgent-s120Gœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      }
    ],
    "nodes": [
      {
        "data": {
          "id": "CustomComponent-PB269",
          "node": {
            "base_classes": [
              "Tool"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Production-ready hybrid search combining vector similarity + keyword matching using Reciprocal Rank Fusion",
            "display_name": "Hybrid Vector Search (RRF)",
            "documentation": "",
            "edited": true,
            "field_order": [
              "connection_string",
              "embedding_model",
              "limit",
              "candidate_pool_size",
              "vector_weight",
              "rrf_k",
              "search_mode"
            ],
            "frozen": false,
            "icon": "search",
            "legacy": false,
            "lf_version": "1.7.2",
            "metadata": {
              "code_hash": "3bd74d85fa09",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "langflow",
                    "version": "0.7.2"
                  },
                  {
                    "name": "langchain",
                    "version": "0.3.23"
                  },
                  {
                    "name": "pydantic",
                    "version": "2.11.10"
                  },
                  {
                    "name": "psycopg",
                    "version": null
                  }
                ],
                "total_dependencies": 4
              },
              "module": "custom_components.hybrid_vector_search_rrf"
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Search Tool",
                "group_outputs": false,
                "hidden": null,
                "loop_types": null,
                "method": "build_tool",
                "name": "tool",
                "options": null,
                "required_inputs": null,
                "selected": "Tool",
                "tool_mode": true,
                "types": [
                  "Tool"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "candidate_pool_size": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Candidate Pool Size",
                "dynamic": false,
                "info": "Number of candidates to retrieve before RRF fusion (higher = better quality, slower)",
                "list": false,
                "list_add_label": "Add More",
                "name": "candidate_pool_size",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 20
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "\"\"\"\r\nEnhanced Hybrid Vector Search Tool for Langflow\r\nImplements RRF (Reciprocal Rank Fusion) combining vector + keyword search\r\nUses modern psycopg 3.x library with built-in connection pooling\r\n\"\"\"\r\n\r\nfrom langflow.custom import Component\r\nfrom langflow.io import Output, SecretStrInput, IntInput, HandleInput, FloatInput, DropdownInput\r\nfrom langflow.field_typing import Tool, Embeddings\r\nfrom langchain.tools import StructuredTool\r\nfrom pydantic import BaseModel, Field\r\nfrom typing import Optional, List, Dict, Any\r\nimport psycopg\r\nfrom psycopg.rows import dict_row\r\nimport json\r\nimport threading\r\nfrom contextlib import contextmanager\r\n\r\n\r\nclass HybridVectorSearchTool(Component):\r\n    display_name = \"Hybrid Vector Search (RRF)\"\r\n    description = \"Production-ready hybrid search combining vector similarity + keyword matching using Reciprocal Rank Fusion\"\r\n    icon = \"search\"\r\n    name = \"HybridVectorTool\"\r\n\r\n    inputs = [\r\n        SecretStrInput(\r\n            name=\"connection_string\",\r\n            display_name=\"Database Connection String\",\r\n            info=\"PostgreSQL connection string (use environment variable for security)\",\r\n            required=True\r\n        ),\r\n        HandleInput(\r\n            name=\"embedding_model\",\r\n            display_name=\"Embedding Model\",\r\n            input_types=[\"Embeddings\"],\r\n            info=\"Connect Google, OpenAI, or Ollama Embeddings component\",\r\n            required=True\r\n        ),\r\n        IntInput(\r\n            name=\"limit\",\r\n            display_name=\"Result Limit\",\r\n            value=5,\r\n            info=\"Maximum number of results to return\"\r\n        ),\r\n        IntInput(\r\n            name=\"candidate_pool_size\",\r\n            display_name=\"Candidate Pool Size\",\r\n            value=20,\r\n            advanced=True,\r\n            info=\"Number of candidates to retrieve before RRF fusion (higher = better quality, slower)\"\r\n        ),\r\n        FloatInput(\r\n            name=\"vector_weight\",\r\n            display_name=\"Vector Search Weight\",\r\n            value=0.5,\r\n            advanced=True,\r\n            info=\"Weight for vector search in RRF (0-1, 0.5 = equal weighting)\"\r\n        ),\r\n        FloatInput(\r\n            name=\"rrf_k\",\r\n            display_name=\"RRF K Parameter\",\r\n            value=60.0,\r\n            advanced=True,\r\n            info=\"Reciprocal Rank Fusion constant (typical range: 1-100)\"\r\n        ),\r\n        DropdownInput(\r\n            name=\"search_mode\",\r\n            display_name=\"Search Mode\",\r\n            options=[\"hybrid\", \"vector_only\", \"keyword_only\"],\r\n            value=\"hybrid\",\r\n            advanced=True,\r\n            info=\"Search strategy to use\"\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(name=\"tool\", display_name=\"Search Tool\", method=\"build_tool\"),\r\n    ]\r\n\r\n    def __init__(self, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self._lock = threading.Lock()\r\n\r\n    @contextmanager\r\n    def get_db_connection(self):\r\n        \"\"\"Context manager for database connections using psycopg 3.x\"\"\"\r\n        conn = None\r\n        try:\r\n            conn = psycopg.connect(self.connection_string, row_factory=dict_row)\r\n            conn.autocommit = False\r\n            yield conn\r\n        finally:\r\n            if conn:\r\n                conn.close()\r\n\r\n    def hybrid_search_query(self, query_embedding: List[float], query_text: str, \r\n                           limit: int, pool_size: int, rrf_k: float) -> str:\r\n        \"\"\"Generate RRF hybrid search SQL\"\"\"\r\n        return \"\"\"\r\n        WITH vector_search AS (\r\n            SELECT \r\n                id, name, type, content, summary, metadata,\r\n                1 - (embedding <=> %s::vector) as similarity_score,\r\n                ROW_NUMBER() OVER (ORDER BY embedding <=> %s::vector) as vector_rank\r\n            FROM nodes\r\n            WHERE embedding IS NOT NULL \r\n              AND is_active = TRUE\r\n            ORDER BY embedding <=> %s::vector\r\n            LIMIT %s\r\n        ),\r\n        keyword_search AS (\r\n            SELECT \r\n                id, name, type, content, summary, metadata,\r\n                ts_rank(\r\n                    to_tsvector('english', COALESCE(name, '') || ' ' || COALESCE(content, '') || ' ' || COALESCE(summary, '')),\r\n                    plainto_tsquery('english', %s)\r\n                ) as keyword_score,\r\n                ROW_NUMBER() OVER (\r\n                    ORDER BY ts_rank(\r\n                        to_tsvector('english', COALESCE(name, '') || ' ' || COALESCE(content, '') || ' ' || COALESCE(summary, '')),\r\n                        plainto_tsquery('english', %s)\r\n                    ) DESC\r\n                ) as keyword_rank\r\n            FROM nodes\r\n            WHERE to_tsvector('english', COALESCE(name, '') || ' ' || COALESCE(content, '') || ' ' || COALESCE(summary, ''))\r\n                  @@ plainto_tsquery('english', %s)\r\n              AND is_active = TRUE\r\n            ORDER BY keyword_score DESC\r\n            LIMIT %s\r\n        ),\r\n        rrf_fusion AS (\r\n            SELECT \r\n                COALESCE(v.id, k.id) as id,\r\n                COALESCE(v.name, k.name) as name,\r\n                COALESCE(v.type, k.type) as type,\r\n                COALESCE(v.content, k.content) as content,\r\n                COALESCE(v.summary, k.summary) as summary,\r\n                COALESCE(v.metadata, k.metadata) as metadata,\r\n                v.similarity_score,\r\n                k.keyword_score,\r\n                v.vector_rank,\r\n                k.keyword_rank,\r\n                -- Reciprocal Rank Fusion formula\r\n                (\r\n                    COALESCE(1.0 / (%s + v.vector_rank), 0.0) + \r\n                    COALESCE(1.0 / (%s + k.keyword_rank), 0.0)\r\n                ) as rrf_score\r\n            FROM vector_search v\r\n            FULL OUTER JOIN keyword_search k ON v.id = k.id\r\n        ),\r\n        enriched_results AS (\r\n            SELECT \r\n                r.*,\r\n                -- Get relationship count for context\r\n                (\r\n                    SELECT COUNT(*) \r\n                    FROM edges e \r\n                    WHERE (e.source_id = r.id OR e.target_id = r.id) \r\n                      AND e.is_active = TRUE\r\n                ) as connection_count\r\n            FROM rrf_fusion r\r\n        )\r\n        SELECT \r\n            id, name, type, content, summary, metadata,\r\n            similarity_score, keyword_score, rrf_score, connection_count\r\n        FROM enriched_results\r\n        ORDER BY rrf_score DESC\r\n        LIMIT %s;\r\n        \"\"\"\r\n\r\n    def vector_only_query(self, query_embedding: List[float], limit: int) -> str:\r\n        \"\"\"Generate vector-only search SQL\"\"\"\r\n        return \"\"\"\r\n        SELECT \r\n            id, name, type, content, summary, metadata,\r\n            1 - (embedding <=> %s::vector) as similarity_score,\r\n            NULL as keyword_score,\r\n            1 - (embedding <=> %s::vector) as rrf_score,\r\n            (\r\n                SELECT COUNT(*) \r\n                FROM edges e \r\n                WHERE (e.source_id = nodes.id OR e.target_id = nodes.id) \r\n                  AND e.is_active = TRUE\r\n            ) as connection_count\r\n        FROM nodes\r\n        WHERE embedding IS NOT NULL \r\n          AND is_active = TRUE\r\n        ORDER BY embedding <=> %s::vector\r\n        LIMIT %s;\r\n        \"\"\"\r\n\r\n    def keyword_only_query(self, query_text: str, limit: int) -> str:\r\n        \"\"\"Generate keyword-only search SQL\"\"\"\r\n        return \"\"\"\r\n        SELECT \r\n            id, name, type, content, summary, metadata,\r\n            NULL as similarity_score,\r\n            ts_rank(\r\n                to_tsvector('english', COALESCE(name, '') || ' ' || COALESCE(content, '') || ' ' || COALESCE(summary, '')),\r\n                plainto_tsquery('english', %s)\r\n            ) as keyword_score,\r\n            ts_rank(\r\n                to_tsvector('english', COALESCE(name, '') || ' ' || COALESCE(content, '') || ' ' || COALESCE(summary, '')),\r\n                plainto_tsquery('english', %s)\r\n            ) as rrf_score,\r\n            (\r\n                SELECT COUNT(*) \r\n                FROM edges e \r\n                WHERE (e.source_id = nodes.id OR e.target_id = nodes.id) \r\n                  AND e.is_active = TRUE\r\n            ) as connection_count\r\n        FROM nodes\r\n        WHERE to_tsvector('english', COALESCE(name, '') || ' ' || COALESCE(content, '') || ' ' || COALESCE(summary, ''))\r\n              @@ plainto_tsquery('english', %s)\r\n          AND is_active = TRUE\r\n        ORDER BY keyword_score DESC\r\n        LIMIT %s;\r\n        \"\"\"\r\n\r\n    def format_results(self, results: List[Dict[str, Any]]) -> str:\r\n        \"\"\"Format search results for LLM consumption\"\"\"\r\n        if not results:\r\n            return \"No relevant results found in the knowledge base.\"\r\n\r\n        formatted_parts = []\r\n        for idx, result in enumerate(results, 1):\r\n            parts = [f\"\\n--- Result {idx} ---\"]\r\n            parts.append(f\"Entity: {result['name']} (Type: {result['type']})\")\r\n            \r\n            if result.get('summary'):\r\n                parts.append(f\"Summary: {result['summary']}\")\r\n            \r\n            if result.get('content'):\r\n                content = result['content']\r\n                if len(content) > 500:\r\n                    content = content[:497] + \"...\"\r\n                parts.append(f\"Content: {content}\")\r\n            \r\n            # Add metadata if present\r\n            if result.get('metadata') and result['metadata']:\r\n                try:\r\n                    metadata = result['metadata'] if isinstance(result['metadata'], dict) else json.loads(result['metadata'])\r\n                    if metadata:\r\n                        parts.append(f\"Metadata: {json.dumps(metadata, indent=2)}\")\r\n                except (json.JSONDecodeError, TypeError):\r\n                    pass\r\n            \r\n            # Add relevance scores\r\n            scores = []\r\n            if result.get('similarity_score') is not None:\r\n                scores.append(f\"Similarity: {result['similarity_score']:.3f}\")\r\n            if result.get('keyword_score') is not None:\r\n                scores.append(f\"Keyword: {result['keyword_score']:.3f}\")\r\n            if result.get('rrf_score') is not None:\r\n                scores.append(f\"Combined: {result['rrf_score']:.3f}\")\r\n            \r\n            if scores:\r\n                parts.append(f\"Relevance: {' | '.join(scores)}\")\r\n            \r\n            if result.get('connection_count'):\r\n                parts.append(f\"Connections: {result['connection_count']} related entities\")\r\n            \r\n            formatted_parts.append(\"\\n\".join(parts))\r\n        \r\n        return \"\\n\".join(formatted_parts)\r\n\r\n    def build_tool(self) -> Tool:\r\n        \"\"\"Build the LangChain Tool\"\"\"\r\n        \r\n        class SearchInput(BaseModel):\r\n            query: str = Field(\r\n                ..., \r\n                description=\"The search query - can be a question, topic, or entity name\"\r\n            )\r\n            search_type: Optional[str] = Field(\r\n                default=\"hybrid\",\r\n                description=\"Search mode: 'hybrid' (default), 'vector_only', or 'keyword_only'\"\r\n            )\r\n\r\n        def execute_search(query: str, search_type: str = \"hybrid\") -> str:\r\n            \"\"\"Execute the hybrid search\"\"\"\r\n            embed_model = self.embedding_model\r\n            if not embed_model:\r\n                return \"Error: No embedding model connected to the search tool.\"\r\n\r\n            try:\r\n                # Determine search mode\r\n                mode = search_type if search_type in [\"hybrid\", \"vector_only\", \"keyword_only\"] else self.search_mode\r\n                \r\n                # Generate embedding if needed\r\n                query_embedding = None\r\n                if mode in [\"hybrid\", \"vector_only\"]:\r\n                    query_embedding = embed_model.embed_query(query)\r\n                \r\n                # Select appropriate query\r\n                with self.get_db_connection() as conn:\r\n                    with conn.cursor() as cur:\r\n                        if mode == \"vector_only\":\r\n                            sql_query = self.vector_only_query(query_embedding, self.limit)\r\n                            params = [query_embedding, query_embedding, query_embedding, self.limit]\r\n                        elif mode == \"keyword_only\":\r\n                            sql_query = self.keyword_only_query(query, self.limit)\r\n                            params = [query, query, query, self.limit]\r\n                        else:  # hybrid\r\n                            sql_query = self.hybrid_search_query(\r\n                                query_embedding, query, \r\n                                self.limit, self.candidate_pool_size, self.rrf_k\r\n                            )\r\n                            params = [\r\n                                query_embedding, query_embedding, query_embedding,  # vector search params\r\n                                self.candidate_pool_size,\r\n                                query, query, query,  # keyword search params\r\n                                self.candidate_pool_size,\r\n                                self.rrf_k, self.rrf_k,  # RRF k parameters\r\n                                self.limit\r\n                            ]\r\n                        \r\n                        cur.execute(sql_query, params)\r\n                        results = cur.fetchall()\r\n                \r\n                return self.format_results(results)\r\n                \r\n            except Exception as e:\r\n                error_msg = f\"Search error: {str(e)}\"\r\n                # Log error for debugging\r\n                print(f\"[HybridVectorTool Error] {error_msg}\")\r\n                return error_msg\r\n\r\n        return StructuredTool.from_function(\r\n            func=execute_search,\r\n            name=\"semantic_search\",\r\n            description=(\r\n                \"Search the knowledge base using semantic understanding. \"\r\n                \"Use this for finding entities, concepts, or information based on meaning, \"\r\n                \"descriptions, topics, or when you don't know exact entity names. \"\r\n                \"Combines vector similarity with keyword matching for best results.\"\r\n            ),\r\n            args_schema=SearchInput\r\n        )"
              },
              "connection_string": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "Database Connection String",
                "dynamic": false,
                "info": "PostgreSQL connection string (use environment variable for security)",
                "input_types": [],
                "load_from_db": false,
                "name": "connection_string",
                "override_skip": false,
                "password": true,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "embedding_model": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Embedding Model",
                "dynamic": false,
                "info": "Connect Google, OpenAI, or Ollama Embeddings component",
                "input_types": [
                  "Embeddings"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "embedding_model",
                "override_skip": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "other",
                "value": ""
              },
              "limit": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Result Limit",
                "dynamic": false,
                "info": "Maximum number of results to return",
                "list": false,
                "list_add_label": "Add More",
                "name": "limit",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 5
              },
              "rrf_k": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "RRF K Parameter",
                "dynamic": false,
                "info": "Reciprocal Rank Fusion constant (typical range: 1-100)",
                "list": false,
                "list_add_label": "Add More",
                "name": "rrf_k",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "float",
                "value": 60
              },
              "search_mode": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Search Mode",
                "dynamic": false,
                "external_options": {},
                "info": "Search strategy to use",
                "name": "search_mode",
                "options": [
                  "hybrid",
                  "vector_only",
                  "keyword_only"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "hybrid"
              },
              "vector_weight": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "Vector Search Weight",
                "dynamic": false,
                "info": "Weight for vector search in RRF (0-1, 0.5 = equal weighting)",
                "list": false,
                "list_add_label": "Add More",
                "name": "vector_weight",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "float",
                "value": 0.5
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "HybridVectorTool"
        },
        "dragging": false,
        "id": "CustomComponent-PB269",
        "measured": {
          "height": 361,
          "width": 320
        },
        "position": {
          "x": -293.0000000000001,
          "y": 497.5000000000001
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "Google Generative AI Embeddings-OtCn3",
          "node": {
            "base_classes": [
              "Embeddings"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Connect to Google's generative AI embeddings service using the GoogleGenerativeAIEmbeddings class, found in the langchain-google-genai package.",
            "display_name": "Google Generative AI Embeddings",
            "documentation": "https://python.langchain.com/v0.2/docs/integrations/text_embedding/google_generative_ai/",
            "edited": false,
            "field_order": [
              "api_key",
              "model_name"
            ],
            "frozen": false,
            "icon": "GoogleGenerativeAI",
            "legacy": false,
            "lf_version": "1.7.2",
            "metadata": {
              "code_hash": "7756ad70a221",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "google",
                    "version": "3.40.0"
                  },
                  {
                    "name": "langchain_core",
                    "version": "0.3.82"
                  },
                  {
                    "name": "langchain_google_genai",
                    "version": "2.0.6"
                  },
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  }
                ],
                "total_dependencies": 4
              },
              "module": "lfx.components.google.google_generative_ai_embeddings.GoogleGenerativeAIEmbeddingsComponent"
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Embeddings",
                "group_outputs": false,
                "method": "build_embeddings",
                "name": "embeddings",
                "selected": "Embeddings",
                "tool_mode": true,
                "types": [
                  "Embeddings"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "Google Generative AI API Key",
                "dynamic": false,
                "info": "",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "override_skip": false,
                "password": true,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "# from lfx.field_typing import Data\n\n# TODO: remove ignore once the google package is published with types\nfrom google.ai.generativelanguage_v1beta.types import BatchEmbedContentsRequest\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\nfrom langchain_google_genai._common import GoogleGenerativeAIError\n\nfrom lfx.custom.custom_component.component import Component\nfrom lfx.io import MessageTextInput, Output, SecretStrInput\n\nMIN_DIMENSION_ERROR = \"Output dimensionality must be at least 1\"\nMAX_DIMENSION_ERROR = (\n    \"Output dimensionality cannot exceed 768. Google's embedding models only support dimensions up to 768.\"\n)\nMAX_DIMENSION = 768\nMIN_DIMENSION = 1\n\n\nclass GoogleGenerativeAIEmbeddingsComponent(Component):\n    display_name = \"Google Generative AI Embeddings\"\n    description = (\n        \"Connect to Google's generative AI embeddings service using the GoogleGenerativeAIEmbeddings class, \"\n        \"found in the langchain-google-genai package.\"\n    )\n    documentation: str = \"https://python.langchain.com/v0.2/docs/integrations/text_embedding/google_generative_ai/\"\n    icon = \"GoogleGenerativeAI\"\n    name = \"Google Generative AI Embeddings\"\n\n    inputs = [\n        SecretStrInput(name=\"api_key\", display_name=\"Google Generative AI API Key\", required=True),\n        MessageTextInput(name=\"model_name\", display_name=\"Model Name\", value=\"models/text-embedding-004\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        if not self.api_key:\n            msg = \"API Key is required\"\n            raise ValueError(msg)\n\n        class HotaGoogleGenerativeAIEmbeddings(GoogleGenerativeAIEmbeddings):\n            def __init__(self, *args, **kwargs) -> None:\n                super(GoogleGenerativeAIEmbeddings, self).__init__(*args, **kwargs)\n\n            def embed_documents(\n                self,\n                texts: list[str],\n                *,\n                batch_size: int = 100,\n                task_type: str | None = None,\n                titles: list[str] | None = None,\n                output_dimensionality: int | None = 768,\n            ) -> list[list[float]]:\n                \"\"\"Embed a list of strings.\n\n                Google Generative AI currently sets a max batch size of 100 strings.\n\n                Args:\n                    texts: List[str] The list of strings to embed.\n                    batch_size: [int] The batch size of embeddings to send to the model\n                    task_type: task_type (https://ai.google.dev/api/rest/v1/TaskType)\n                    titles: An optional list of titles for texts provided.\n                    Only applicable when TaskType is RETRIEVAL_DOCUMENT.\n                    output_dimensionality: Optional reduced dimension for the output embedding.\n                    https://ai.google.dev/api/rest/v1/models/batchEmbedContents#EmbedContentRequest\n                Returns:\n                    List of embeddings, one for each text.\n                \"\"\"\n                if output_dimensionality is not None and output_dimensionality < MIN_DIMENSION:\n                    raise ValueError(MIN_DIMENSION_ERROR)\n                if output_dimensionality is not None and output_dimensionality > MAX_DIMENSION:\n                    error_msg = MAX_DIMENSION_ERROR.format(output_dimensionality)\n                    raise ValueError(error_msg)\n\n                embeddings: list[list[float]] = []\n                batch_start_index = 0\n                for batch in GoogleGenerativeAIEmbeddings._prepare_batches(texts, batch_size):\n                    if titles:\n                        titles_batch = titles[batch_start_index : batch_start_index + len(batch)]\n                        batch_start_index += len(batch)\n                    else:\n                        titles_batch = [None] * len(batch)  # type: ignore[list-item]\n\n                    requests = [\n                        self._prepare_request(\n                            text=text,\n                            task_type=task_type,\n                            title=title,\n                            output_dimensionality=output_dimensionality,\n                        )\n                        for text, title in zip(batch, titles_batch, strict=True)\n                    ]\n\n                    try:\n                        result = self.client.batch_embed_contents(\n                            BatchEmbedContentsRequest(requests=requests, model=self.model)\n                        )\n                    except Exception as e:\n                        msg = f\"Error embedding content: {e}\"\n                        raise GoogleGenerativeAIError(msg) from e\n                    embeddings.extend([list(e.values) for e in result.embeddings])\n                return embeddings\n\n            def embed_query(\n                self,\n                text: str,\n                task_type: str | None = None,\n                title: str | None = None,\n                output_dimensionality: int | None = 768,\n            ) -> list[float]:\n                \"\"\"Embed a text.\n\n                Args:\n                    text: The text to embed.\n                    task_type: task_type (https://ai.google.dev/api/rest/v1/TaskType)\n                    title: An optional title for the text.\n                    Only applicable when TaskType is RETRIEVAL_DOCUMENT.\n                    output_dimensionality: Optional reduced dimension for the output embedding.\n                    https://ai.google.dev/api/rest/v1/models/batchEmbedContents#EmbedContentRequest\n\n                Returns:\n                    Embedding for the text.\n                \"\"\"\n                if output_dimensionality is not None and output_dimensionality < MIN_DIMENSION:\n                    raise ValueError(MIN_DIMENSION_ERROR)\n                if output_dimensionality is not None and output_dimensionality > MAX_DIMENSION:\n                    error_msg = MAX_DIMENSION_ERROR.format(output_dimensionality)\n                    raise ValueError(error_msg)\n\n                task_type = task_type or \"RETRIEVAL_QUERY\"\n                return self.embed_documents(\n                    [text],\n                    task_type=task_type,\n                    titles=[title] if title else None,\n                    output_dimensionality=output_dimensionality,\n                )[0]\n\n        return HotaGoogleGenerativeAIEmbeddings(model=self.model_name, google_api_key=self.api_key)\n"
              },
              "model_name": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Model Name",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "model_name",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "models/text-embedding-004"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "Google Generative AI Embeddings"
        },
        "dragging": false,
        "id": "Google Generative AI Embeddings-OtCn3",
        "measured": {
          "height": 333,
          "width": 320
        },
        "position": {
          "x": -784.0000000000001,
          "y": 379
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "CustomComponent-ZCAlX",
          "node": {
            "base_classes": [
              "Text",
              "Tool"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Query knowledge graph with multi-hop traversal, path finding, and relationship analysis",
            "display_name": "Advanced Graph Query Tool",
            "documentation": "",
            "edited": true,
            "field_order": [
              "connection_string",
              "entity_name",
              "max_hops",
              "result_limit",
              "query_type"
            ],
            "frozen": false,
            "icon": "network",
            "legacy": false,
            "lf_version": "1.7.2",
            "metadata": {
              "code_hash": "3ba46db7e39a",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "langflow",
                    "version": "0.7.2"
                  },
                  {
                    "name": "langchain",
                    "version": "0.3.23"
                  },
                  {
                    "name": "pydantic",
                    "version": "2.11.10"
                  },
                  {
                    "name": "psycopg",
                    "version": null
                  }
                ],
                "total_dependencies": 4
              },
              "module": "custom_components.advanced_graph_query_tool"
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Graph Tool",
                "group_outputs": false,
                "hidden": null,
                "loop_types": null,
                "method": "build_tool",
                "name": "tool",
                "options": null,
                "required_inputs": null,
                "selected": "Tool",
                "tool_mode": true,
                "types": [
                  "Tool"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Direct Result",
                "group_outputs": false,
                "hidden": null,
                "loop_types": null,
                "method": "search_graph",
                "name": "result",
                "options": null,
                "required_inputs": null,
                "selected": "Text",
                "tool_mode": true,
                "types": [
                  "Text"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "\"\"\"\r\nAdvanced Knowledge Graph Query Tool for Langflow (Simplified)\r\nSupports multi-hop traversal, path finding, and graph analytics\r\n\"\"\"\r\n\r\nfrom langflow.custom import Component\r\nfrom langflow.io import Output, SecretStrInput, IntInput, MessageTextInput, DropdownInput\r\nfrom langflow.field_typing import Tool\r\nfrom langchain.tools import StructuredTool\r\nfrom pydantic import BaseModel, Field\r\nfrom typing import Optional, List, Dict, Any\r\nimport psycopg\r\nfrom psycopg.rows import dict_row\r\nimport json\r\n\r\n\r\nclass AdvancedGraphQueryTool(Component):\r\n    display_name = \"Advanced Graph Query Tool\"\r\n    description = \"Query knowledge graph with multi-hop traversal, path finding, and relationship analysis\"\r\n    icon = \"network\"\r\n    name = \"AdvancedGraphTool\"\r\n\r\n    inputs = [\r\n        SecretStrInput(\r\n            name=\"connection_string\",\r\n            display_name=\"Database Connection String\",\r\n            info=\"PostgreSQL connection string (e.g., postgresql://user:pass@localhost:5432/db)\",\r\n            required=True\r\n        ),\r\n        MessageTextInput(\r\n            name=\"entity_name\",\r\n            display_name=\"Entity Name (Optional)\",\r\n            info=\"Entity to search for when using standalone mode\"\r\n        ),\r\n        IntInput(\r\n            name=\"max_hops\",\r\n            display_name=\"Max Traversal Depth\",\r\n            value=3,\r\n            info=\"Maximum depth for graph traversal (1-5 recommended)\"\r\n        ),\r\n        IntInput(\r\n            name=\"result_limit\",\r\n            display_name=\"Result Limit\",\r\n            value=10,\r\n            info=\"Maximum results per query\"\r\n        ),\r\n        DropdownInput(\r\n            name=\"query_type\",\r\n            display_name=\"Query Type\",\r\n            options=[\"relationships\", \"multi_hop\", \"shortest_path\", \"subgraph\", \"importance\"],\r\n            value=\"relationships\",\r\n            advanced=True,\r\n            info=\"Type of graph query to execute\"\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(name=\"tool\", display_name=\"Graph Tool\", method=\"build_tool\"),\r\n        Output(name=\"result\", display_name=\"Direct Result\", method=\"search_graph\"),\r\n    ]\r\n\r\n    def _get_connection(self):\r\n        \"\"\"Helper to create a new database connection.\"\"\"\r\n        return psycopg.connect(self.connection_string, row_factory=dict_row)\r\n\r\n    def get_direct_relationships(self, entity_name: str, limit: int) -> List[Dict]:\r\n        query = \"\"\"\r\n        SELECT \r\n            CASE \r\n                WHEN e.source_id = n.id THEN 'outgoing'\r\n                ELSE 'incoming'\r\n            END AS direction,\r\n            n.name AS source_entity,\r\n            n.type AS source_type,\r\n            e.relationship_type,\r\n            e.weight,\r\n            target_node.name AS related_entity,\r\n            target_node.type AS related_type,\r\n            target_node.summary AS related_summary,\r\n            target_node.content AS related_content\r\n        FROM nodes n\r\n        JOIN edges e ON (e.source_id = n.id OR e.target_id = n.id)\r\n        JOIN nodes target_node ON (\r\n            CASE \r\n                WHEN e.source_id = n.id THEN e.target_id\r\n                ELSE e.source_id\r\n            END = target_node.id\r\n        )\r\n        WHERE n.name = %s\r\n          AND n.is_active = TRUE\r\n          AND target_node.is_active = TRUE\r\n          AND e.is_active = TRUE\r\n        ORDER BY e.weight DESC NULLS LAST, direction, related_entity\r\n        LIMIT %s;\r\n        \"\"\"\r\n        with self._get_connection() as conn:\r\n            with conn.cursor() as cur:\r\n                cur.execute(query, (entity_name, limit))\r\n                return cur.fetchall()\r\n\r\n    def get_multi_hop_traversal(self, entity_name: str, max_depth: int, limit: int) -> List[Dict]:\r\n        query = \"\"\"\r\n        WITH RECURSIVE paths AS (\r\n            SELECT \r\n                e.source_id,\r\n                e.target_id,\r\n                e.relationship_type,\r\n                ARRAY[source.name] as path_names,\r\n                ARRAY[source.type] as path_types,\r\n                ARRAY[e.relationship_type] as path_relations,\r\n                ARRAY[e.source_id] as path_ids,\r\n                1 as depth,\r\n                target.name as target_name,\r\n                target.type as target_type,\r\n                target.summary as target_summary,\r\n                target.content as target_content\r\n            FROM edges e\r\n            JOIN nodes source ON e.source_id = source.id\r\n            JOIN nodes target ON e.target_id = target.id\r\n            WHERE source.name = %s\r\n              AND source.is_active = TRUE\r\n              AND target.is_active = TRUE\r\n              AND e.is_active = TRUE\r\n            \r\n            UNION ALL\r\n            \r\n            SELECT \r\n                p.source_id,\r\n                e.target_id,\r\n                e.relationship_type,\r\n                p.path_names || target.name,\r\n                p.path_types || target.type,\r\n                p.path_relations || e.relationship_type,\r\n                p.path_ids || e.target_id,\r\n                p.depth + 1,\r\n                target.name,\r\n                target.type,\r\n                target.summary,\r\n                target.content\r\n            FROM paths p\r\n            JOIN edges e ON p.target_id = e.source_id\r\n            JOIN nodes target ON e.target_id = target.id\r\n            WHERE p.depth < %s\r\n              AND NOT e.target_id = ANY(p.path_ids)\r\n              AND target.is_active = TRUE\r\n              AND e.is_active = TRUE\r\n        )\r\n        SELECT DISTINCT\r\n            path_names,\r\n            path_types,\r\n            path_relations,\r\n            depth,\r\n            target_name,\r\n            target_type,\r\n            target_summary,\r\n            target_content\r\n        FROM paths\r\n        ORDER BY depth, target_name\r\n        LIMIT %s;\r\n        \"\"\"\r\n        with self._get_connection() as conn:\r\n            with conn.cursor() as cur:\r\n                cur.execute(query, (entity_name, max_depth, limit))\r\n                return cur.fetchall()\r\n\r\n    def find_shortest_path(self, start_entity: str, end_entity: str, max_depth: int) -> List[Dict]:\r\n        query = \"\"\"\r\n        WITH RECURSIVE path_search AS (\r\n            SELECT \r\n                e.source_id,\r\n                e.target_id,\r\n                e.relationship_type,\r\n                ARRAY[sn.name, tn.name] as node_names,\r\n                ARRAY[e.relationship_type] as relationships,\r\n                ARRAY[e.source_id, e.target_id] as node_ids,\r\n                1 as depth\r\n            FROM edges e\r\n            JOIN nodes sn ON e.source_id = sn.id\r\n            JOIN nodes tn ON e.target_id = tn.id\r\n            WHERE sn.name = %s\r\n              AND sn.is_active = TRUE\r\n              AND tn.is_active = TRUE\r\n              AND e.is_active = TRUE\r\n            \r\n            UNION ALL\r\n            \r\n            SELECT \r\n                p.source_id,\r\n                e.target_id,\r\n                e.relationship_type,\r\n                p.node_names || tn.name,\r\n                p.relationships || e.relationship_type,\r\n                p.node_ids || e.target_id,\r\n                p.depth + 1\r\n            FROM path_search p\r\n            JOIN edges e ON p.target_id = e.source_id\r\n            JOIN nodes tn ON e.target_id = tn.id\r\n            WHERE p.depth < %s\r\n              AND NOT e.target_id = ANY(p.node_ids)\r\n              AND tn.is_active = TRUE\r\n              AND e.is_active = TRUE\r\n        )\r\n        SELECT \r\n            node_names as path,\r\n            relationships,\r\n            depth as path_length\r\n        FROM path_search\r\n        WHERE node_names[array_length(node_names, 1)] = %s\r\n        ORDER BY depth\r\n        LIMIT 1;\r\n        \"\"\"\r\n        with self._get_connection() as conn:\r\n            with conn.cursor() as cur:\r\n                cur.execute(query, (start_entity, max_depth, end_entity))\r\n                return cur.fetchall()\r\n\r\n    def get_entity_subgraph(self, entity_name: str, depth: int) -> List[Dict]:\r\n        query = \"\"\"\r\n        WITH RECURSIVE subgraph AS (\r\n            SELECT id, name, type, 0 as depth, ARRAY[id] as visited\r\n            FROM nodes\r\n            WHERE name = %s AND is_active = TRUE\r\n            \r\n            UNION\r\n            \r\n            SELECT \r\n                n.id, n.name, n.type, s.depth + 1,\r\n                s.visited || n.id\r\n            FROM subgraph s\r\n            JOIN edges e ON (s.id = e.source_id OR s.id = e.target_id)\r\n            JOIN nodes n ON (\r\n                CASE \r\n                    WHEN s.id = e.source_id THEN e.target_id\r\n                    ELSE e.source_id\r\n                END = n.id\r\n            )\r\n            WHERE s.depth < %s\r\n              AND NOT n.id = ANY(s.visited)\r\n              AND n.is_active = TRUE\r\n              AND e.is_active = TRUE\r\n        )\r\n        SELECT \r\n            sg.name, sg.type, sg.depth,\r\n            COALESCE(json_agg(\r\n                json_build_object(\r\n                    'target', target.name,\r\n                    'relationship', e.relationship_type,\r\n                    'direction', CASE WHEN e.source_id = sg.id THEN 'outgoing' ELSE 'incoming' END\r\n                )\r\n            ) FILTER (WHERE e.id IS NOT NULL), '[]'::json) as relationships\r\n        FROM subgraph sg\r\n        LEFT JOIN edges e ON (sg.id = e.source_id OR sg.id = e.target_id)\r\n        LEFT JOIN nodes target ON (\r\n            CASE \r\n                WHEN sg.id = e.source_id THEN e.target_id\r\n                ELSE e.source_id\r\n            END = target.id\r\n        )\r\n        WHERE target.is_active = TRUE OR target.id IS NULL\r\n        GROUP BY sg.id, sg.name, sg.type, sg.depth\r\n        ORDER BY sg.depth, sg.name;\r\n        \"\"\"\r\n        with self._get_connection() as conn:\r\n            with conn.cursor() as cur:\r\n                cur.execute(query, (entity_name, depth))\r\n                return cur.fetchall()\r\n\r\n    def get_entity_importance(self, entity_name: str) -> List[Dict]:\r\n        query = \"\"\"\r\n        WITH entity_stats AS (\r\n            SELECT \r\n                n.id, n.name, n.type,\r\n                COUNT(DISTINCT e_in.id) as in_degree,\r\n                COUNT(DISTINCT e_out.id) as out_degree,\r\n                COUNT(DISTINCT e_in.id) + COUNT(DISTINCT e_out.id) as total_degree,\r\n                AVG(e_out.weight) as avg_outgoing_weight,\r\n                AVG(e_in.weight) as avg_incoming_weight\r\n            FROM nodes n\r\n            LEFT JOIN edges e_in ON n.id = e_in.target_id AND e_in.is_active = TRUE\r\n            LEFT JOIN edges e_out ON n.id = e_out.source_id AND e_out.is_active = TRUE\r\n            WHERE n.name = %s\r\n              AND n.is_active = TRUE\r\n            GROUP BY n.id, n.name, n.type\r\n        ),\r\n        neighbor_importance AS (\r\n            SELECT \r\n                es.name,\r\n                COALESCE(AVG(neighbor_stats.total_degree), 0) as avg_neighbor_importance\r\n            FROM entity_stats es\r\n            LEFT JOIN edges e ON (es.id = e.source_id OR es.id = e.target_id)\r\n            LEFT JOIN (\r\n                SELECT \r\n                    n2.id,\r\n                    COUNT(DISTINCT e2.id) as total_degree\r\n                FROM nodes n2\r\n                LEFT JOIN edges e2 ON (n2.id = e2.source_id OR n2.id = e2.target_id)\r\n                WHERE n2.is_active = TRUE AND e2.is_active = TRUE\r\n                GROUP BY n2.id\r\n            ) neighbor_stats ON (\r\n                CASE \r\n                    WHEN es.id = e.source_id THEN e.target_id\r\n                    ELSE e.source_id\r\n                END = neighbor_stats.id\r\n            )\r\n            WHERE e.is_active = TRUE\r\n            GROUP BY es.name\r\n        )\r\n        SELECT \r\n            es.*,\r\n            ROUND((es.in_degree::FLOAT / NULLIF(es.out_degree, 0))::NUMERIC, 2) as importance_ratio,\r\n            ROUND(ni.avg_neighbor_importance::NUMERIC, 2) as avg_neighbor_importance\r\n        FROM entity_stats es\r\n        JOIN neighbor_importance ni ON es.name = ni.name;\r\n        \"\"\"\r\n        with self._get_connection() as conn:\r\n            with conn.cursor() as cur:\r\n                cur.execute(query, (entity_name,))\r\n                return cur.fetchall()\r\n\r\n    def format_results(self, results: List[Dict], query_type: str) -> str:\r\n        if not results:\r\n            return f\"No results found for the {query_type} query.\"\r\n\r\n        if query_type == \"relationships\":\r\n            if not results:\r\n                return f\"No relationships found for '{target}'.\"\r\n            \r\n            # Get the central entity name from first result\r\n            central_entity = results[0]['source_entity']\r\n            parts = [f\"Direct relationships for {central_entity}:\\n\"]\r\n            \r\n            for r in results:\r\n                if r['direction'] == 'outgoing':\r\n                    # Standard: Central --[REL]--> Other\r\n                    parts.append(\r\n                        f\"• {r['source_entity']} --[{r['relationship_type']}]--> \"\r\n                        f\"{r['related_entity']} ({r['related_type']})\"\r\n                    )\r\n                else:\r\n                    # Inverse: Other --[REL]--> Central  → show as Central <--[REL]-- Other\r\n                    parts.append(\r\n                        f\"• {r['related_entity']} ({r['related_type']}) \"\r\n                        f\"--[{r['relationship_type']}]--> {r['source_entity']}\"\r\n                    )\r\n                \r\n                if r.get('related_summary'):\r\n                    parts.append(f\"  Summary: {r['related_summary']}\")\r\n            \r\n            return \"\\n\".join(parts)\r\n\r\n        elif query_type == \"multi_hop\":\r\n            parts = [f\"Multi-hop traversal results ({len(results)} paths):\\n\"]\r\n            for r in results:\r\n                path = \" → \".join(r['path_names'])\r\n                relations = \" → \".join(r['path_relations'])\r\n                parts.append(f\"Depth {r['depth']}: {path}\")\r\n                parts.append(f\"  Relations: {relations}\")\r\n                parts.append(f\"  Ends at: {r['target_name']} ({r['target_type']})\")\r\n                if r.get('target_summary'):\r\n                    parts.append(f\"  Summary: {r['target_summary']}\")\r\n                parts.append(\"\")\r\n            return \"\\n\".join(parts)\r\n\r\n        elif query_type == \"shortest_path\":\r\n            if results:\r\n                r = results[0]\r\n                path = \" → \".join(r['path'])\r\n                relations = \" → \".join(r['relationships'])\r\n                return f\"Shortest path (length {r['path_length']}):\\n{path}\\n\\nRelationships:\\n{relations}\"\r\n            return \"No path found between the specified entities.\"\r\n\r\n        elif query_type == \"subgraph\":\r\n            parts = [\"Subgraph structure:\\n\"]\r\n            for r in results:\r\n                parts.append(f\"[Depth {r['depth']}] {r['name']} ({r['type']})\")\r\n                rels = r['relationships']\r\n                if isinstance(rels, str):\r\n                    rels = json.loads(rels)\r\n                for rel in rels:\r\n                    direction = \"→\" if rel['direction'] == 'outgoing' else \"←\"\r\n                    parts.append(f\"  {direction} [{rel['relationship']}] {direction} {rel['target']}\")\r\n            return \"\\n\".join(parts)\r\n\r\n        elif query_type == \"importance\":\r\n            if results:\r\n                r = results[0]\r\n                parts = [\r\n                    f\"Importance metrics for {r['name']}:\",\r\n                    f\"• Type: {r['type']}\",\r\n                    f\"• Incoming connections: {r['in_degree']}\",\r\n                    f\"• Outgoing connections: {r['out_degree']}\",\r\n                    f\"• Total connections: {r['total_degree']}\",\r\n                    f\"• Importance ratio: {r.get('importance_ratio', 'N/A')}\",\r\n                    f\"• Average neighbor importance: {r.get('avg_neighbor_importance', 'N/A')}\"\r\n                ]\r\n                return \"\\n\".join(parts)\r\n\r\n        return str(results)\r\n\r\n    def search_graph(self, entity_name: str = None) -> str:\r\n        target = entity_name or self.entity_name\r\n        if not target:\r\n            return \"Please provide an entity name.\"\r\n\r\n        try:\r\n            if self.query_type == \"relationships\":\r\n                results = self.get_direct_relationships(target, self.result_limit)\r\n            elif self.query_type == \"multi_hop\":\r\n                results = self.get_multi_hop_traversal(target, self.max_hops, self.result_limit)\r\n            elif self.query_type == \"importance\":\r\n                results = self.get_entity_importance(target)\r\n            else:\r\n                results = self.get_direct_relationships(target, self.result_limit)\r\n            \r\n            return self.format_results(results, self.query_type)\r\n        except Exception as e:\r\n            return f\"Graph query error: {str(e)}\"\r\n\r\n    def build_tool(self) -> Tool:\r\n        class GraphQueryInput(BaseModel):\r\n            entity_name: str = Field(..., description=\"The exact name of the entity to query\")\r\n            query_type: Optional[str] = Field(\r\n                default=\"relationships\",\r\n                description=\"Type of query: 'relationships', 'multi_hop', 'shortest_path', 'subgraph', or 'importance'\"\r\n            )\r\n            target_entity: Optional[str] = Field(\r\n                default=None,\r\n                description=\"Target entity for path queries (used with 'shortest_path')\"\r\n            )\r\n\r\n        def execute_query(entity_name: str, query_type: str = \"relationships\", target_entity: str = None) -> str:\r\n            try:\r\n                if query_type == \"shortest_path\" and target_entity:\r\n                    results = self.find_shortest_path(entity_name, target_entity, self.max_hops)\r\n                elif query_type == \"multi_hop\":\r\n                    results = self.get_multi_hop_traversal(entity_name, self.max_hops, self.result_limit)\r\n                elif query_type == \"subgraph\":\r\n                    results = self.get_entity_subgraph(entity_name, self.max_hops)\r\n                elif query_type == \"importance\":\r\n                    results = self.get_entity_importance(entity_name)\r\n                else:\r\n                    results = self.get_direct_relationships(entity_name, self.result_limit)\r\n                \r\n                return self.format_results(results, query_type)\r\n            except Exception as e:\r\n                return f\"Graph query error: {str(e)}\"\r\n\r\n        return StructuredTool.from_function(\r\n            func=execute_query,\r\n            name=\"explore_knowledge_graph\",\r\n            description=(\r\n                \"Explore the knowledge graph structure. Use this to find relationships, \"\r\n                \"hierarchies, connections between entities, organizational structure, \"\r\n                \"or to understand how entities are related. Requires exact entity names. \"\r\n                \"Supports: direct relationships, multi-hop traversal, path finding, and importance analysis.\"\r\n            ),\r\n            args_schema=GraphQueryInput\r\n        )"
              },
              "connection_string": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "Database Connection String",
                "dynamic": false,
                "info": "PostgreSQL connection string (e.g., postgresql://user:pass@localhost:5432/db)",
                "input_types": [],
                "load_from_db": false,
                "name": "connection_string",
                "override_skip": false,
                "password": true,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "entity_name": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Entity Name (Optional)",
                "dynamic": false,
                "info": "Entity to search for when using standalone mode",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "entity_name",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "max_hops": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Max Traversal Depth",
                "dynamic": false,
                "info": "Maximum depth for graph traversal (1-5 recommended)",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_hops",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 3
              },
              "query_type": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Query Type",
                "dynamic": false,
                "external_options": {},
                "info": "Type of graph query to execute",
                "name": "query_type",
                "options": [
                  "relationships",
                  "multi_hop",
                  "shortest_path",
                  "subgraph",
                  "importance"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "relationships"
              },
              "result_limit": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Result Limit",
                "dynamic": false,
                "info": "Maximum results per query",
                "list": false,
                "list_add_label": "Add More",
                "name": "result_limit",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 10
              }
            },
            "tool_mode": false
          },
          "selected_output": "tool",
          "showNode": true,
          "type": "AdvancedGraphTool"
        },
        "dragging": false,
        "id": "CustomComponent-ZCAlX",
        "measured": {
          "height": 465,
          "width": 320
        },
        "position": {
          "x": 409.9999999999999,
          "y": -327.50000000000017
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "ChatInput-19rHM",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get chat inputs from the Playground.",
            "display_name": "Chat Input",
            "documentation": "https://docs.langflow.org/chat-input-and-output",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "context_id",
              "files"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "lf_version": "1.7.2",
            "metadata": {
              "code_hash": "7a26c54d89ed",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  }
                ],
                "total_dependencies": 1
              },
              "module": "lfx.components.input_output.chat.ChatInput"
            },
            "minimized": true,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Chat Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from lfx.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom lfx.base.io.chat import ChatComponent\nfrom lfx.inputs.inputs import BoolInput\nfrom lfx.io import (\n    DropdownInput,\n    FileInput,\n    MessageTextInput,\n    MultilineInput,\n    Output,\n)\nfrom lfx.schema.message import Message\nfrom lfx.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_USER,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    documentation: str = \"https://docs.langflow.org/chat-input-and-output\"\n    icon = \"MessagesSquare\"\n    name = \"ChatInput\"\n    minimized = True\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Input Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n            input_types=[],\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"context_id\",\n            display_name=\"Context ID\",\n            info=\"The context ID of the chat. Adds an extra layer to the local memory.\",\n            value=\"\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n            temp_file=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Chat Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    async def message_response(self) -> Message:\n        # Ensure files is a list and filter out empty/None values\n        files = self.files if self.files else []\n        if files and not isinstance(files, list):\n            files = [files]\n        # Filter out None/empty values\n        files = [f for f in files if f is not None and f != \"\"]\n\n        session_id = self.session_id or self.graph.session_id or \"\"\n        message = await Message.create(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=session_id,\n            context_id=self.context_id,\n            files=files,\n        )\n        if session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = await self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n"
              },
              "context_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Context ID",
                "dynamic": false,
                "info": "The context ID of the chat. Adds an extra layer to the local memory.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "context_id",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "files": {
                "_input_type": "FileInput",
                "advanced": true,
                "display_name": "Files",
                "dynamic": false,
                "fileTypes": [
                  "csv",
                  "json",
                  "pdf",
                  "txt",
                  "md",
                  "mdx",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "jpg",
                  "jpeg",
                  "png",
                  "bmp",
                  "image"
                ],
                "file_path": "",
                "info": "Files to be sent with the message.",
                "list": true,
                "list_add_label": "Add More",
                "name": "files",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "temp_file": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "file",
                "value": ""
              },
              "input_value": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "ai_enabled": false,
                "copy_field": false,
                "display_name": "Input Text",
                "dynamic": false,
                "info": "Message to be passed as input.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Sender Type",
                "dynamic": false,
                "external_options": {},
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "User"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sender_name",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "User"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "session_id",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_store_message",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "showNode": false,
          "type": "ChatInput"
        },
        "id": "ChatInput-19rHM",
        "measured": {
          "height": 48,
          "width": 192
        },
        "position": {
          "x": 61,
          "y": 95
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "LMStudioModel-bVpBg",
          "node": {
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Generate text using LM Studio Local LLMs.",
            "display_name": "LM Studio",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_tokens",
              "model_kwargs",
              "model_name",
              "base_url",
              "api_key",
              "temperature",
              "seed"
            ],
            "frozen": false,
            "icon": "LMStudio",
            "last_updated": "2026-01-11T22:11:03.332Z",
            "legacy": false,
            "lf_version": "1.7.2",
            "metadata": {
              "code_hash": "da4b3b155dc4",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "httpx",
                    "version": "0.28.1"
                  },
                  {
                    "name": "langchain_openai",
                    "version": "0.3.35"
                  },
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  },
                  {
                    "name": "openai",
                    "version": "1.109.1"
                  }
                ],
                "total_dependencies": 4
              },
              "keywords": [
                "model",
                "llm",
                "language model",
                "large language model"
              ],
              "module": "lfx.components.lmstudio.lmstudiomodel.LMStudioModelComponent"
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Model Response",
                "group_outputs": false,
                "loop_types": null,
                "method": "text_response",
                "name": "text_output",
                "options": null,
                "required_inputs": null,
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Language Model",
                "group_outputs": false,
                "loop_types": null,
                "method": "build_model",
                "name": "model_output",
                "options": null,
                "required_inputs": null,
                "selected": "LanguageModel",
                "tool_mode": true,
                "types": [
                  "LanguageModel"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_frontend_node_flow_id": {
                "value": "569a820f-2b87-4c05-84c8-085107a108d6"
              },
              "_frontend_node_folder_id": {
                "value": "67c46809-3dbf-4d90-8f50-9b045df8edc1"
              },
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": true,
                "display_name": "LM Studio API Key",
                "dynamic": false,
                "info": "The LM Studio API Key to use for LM Studio.",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "override_skip": false,
                "password": true,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "base_url": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Base URL",
                "dynamic": false,
                "info": "Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "base_url",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "http://127.0.0.1:8317/v1"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_openai import ChatOpenAI\n\nfrom lfx.base.models.model import LCModelComponent\nfrom lfx.field_typing import LanguageModel\nfrom lfx.field_typing.range_spec import RangeSpec\nfrom lfx.inputs.inputs import DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\n\n\nclass LMStudioModelComponent(LCModelComponent):\n    display_name = \"LM Studio\"\n    description = \"Generate text using LM Studio Local LLMs.\"\n    icon = \"LMStudio\"\n    name = \"LMStudioModel\"\n\n    async def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):  # noqa: ARG002\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = await self.get_variables(base_url_value, field_name)\n            try:\n                async with httpx.AsyncClient() as client:\n                    response = await client.get(urljoin(base_url_value, \"/v1/models\"), timeout=2.0)\n                    response.raise_for_status()\n            except httpx.HTTPError:\n                msg = \"Could not access the default LM Studio URL. Please, specify the 'Base URL' field.\"\n                self.log(msg)\n                return build_config\n            build_config[\"model_name\"][\"options\"] = await self.get_model(base_url_value)\n\n        return build_config\n\n    @staticmethod\n    async def get_model(base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/v1/models\")\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"id\"] for model in data.get(\"data\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure the LM Studio server is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        *LCModelComponent.get_base_inputs(),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            refresh_button=True,\n        ),\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            advanced=False,\n            info=\"Endpoint of the LM Studio API. Defaults to 'http://localhost:1234/v1' if not specified.\",\n            value=\"http://localhost:1234/v1\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"LM Studio API Key\",\n            info=\"The LM Studio API Key to use for LM Studio.\",\n            advanced=True,\n            value=\"LMSTUDIO_API_KEY\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        lmstudio_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        base_url = self.base_url or \"http://localhost:1234/v1\"\n        seed = self.seed\n\n        return ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=base_url,\n            api_key=lmstudio_api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an LM Studio exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "is_refresh": true,
              "max_tokens": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Tokens",
                "dynamic": false,
                "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_tokens",
                "override_skip": false,
                "placeholder": "",
                "range_spec": {
                  "max": 128000,
                  "min": 0,
                  "step": 0.1,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": ""
              },
              "model_kwargs": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Model Kwargs",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "model_kwargs",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "track_in_telemetry": false,
                "type": "dict",
                "value": {}
              },
              "model_name": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Name",
                "dynamic": false,
                "external_options": {},
                "info": "",
                "name": "model_name",
                "options": [
                  "kimi-k2-0905",
                  "kimi-k2",
                  "minimax-m2",
                  "gemini-2.5-computer-use-preview-10-2025",
                  "kiro-claude-haiku-4-5",
                  "kiro-claude-haiku-4-5-agentic",
                  "gpt-5-codex-mini",
                  "gpt-5-mini",
                  "qwen3-max-preview",
                  "gpt-oss-120b-medium",
                  "gpt-5-codex",
                  "kimi-k2-thinking",
                  "gpt-5.1-codex-max",
                  "gpt-5",
                  "claude-sonnet-4",
                  "qwen3-vl-plus",
                  "deepseek-v3",
                  "gemini-claude-sonnet-4-5-thinking",
                  "gemini-2.5-pro",
                  "gemini-3-pro",
                  "qwen3-max",
                  "vision-model",
                  "claude-opus-4.5",
                  "gemini-claude-sonnet-4-5",
                  "gemini-2.5-flash-lite",
                  "deepseek-v3.2-reasoner",
                  "deepseek-r1",
                  "kiro-claude-opus-4-5",
                  "claude-opus-4.1",
                  "kiro-claude-sonnet-4-agentic",
                  "gpt-5.2-codex",
                  "gpt-5.1",
                  "kiro-claude-sonnet-4-5-agentic",
                  "minimax-m2.1",
                  "gemini-3-pro-image-preview",
                  "gpt-5.1-codex-mini",
                  "claude-haiku-4.5",
                  "raptor-mini",
                  "deepseek-v3.2",
                  "qwen3-32b",
                  "kiro-claude-sonnet-4-5",
                  "gemini-3-flash-preview",
                  "grok-code-fast-1",
                  "tstars2.0",
                  "qwen3-coder-plus",
                  "kiro-claude-sonnet-4",
                  "gpt-5.2",
                  "qwen3-235b-a22b-instruct",
                  "gemini-3-pro-preview",
                  "gpt-4.1",
                  "claude-sonnet-4.5",
                  "qwen3-235b",
                  "gemini-claude-opus-4-5-thinking",
                  "glm-4.6",
                  "deepseek-v3.2-chat",
                  "deepseek-v3.1",
                  "qwen3-235b-a22b-thinking-2507",
                  "gpt-5.1-codex",
                  "glm-4.7",
                  "kiro-claude-opus-4-5-agentic",
                  "qwen3-coder-flash",
                  "gemini-2.5-flash"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "refresh_button": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "gemini-2.5-flash"
              },
              "seed": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Seed",
                "dynamic": false,
                "info": "The seed controls the reproducibility of the job.",
                "list": false,
                "list_add_label": "Add More",
                "name": "seed",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 1
              },
              "stream": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Stream",
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "list": false,
                "list_add_label": "Add More",
                "name": "stream",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": false
              },
              "system_message": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "ai_enabled": false,
                "copy_field": false,
                "display_name": "System Message",
                "dynamic": false,
                "info": "System message to pass to the model.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_message",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "temperature": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "Temperature",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "temperature",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "float",
                "value": 0.1
              }
            },
            "tool_mode": false
          },
          "selected_output": "model_output",
          "showNode": true,
          "type": "LMStudioModel"
        },
        "dragging": false,
        "id": "LMStudioModel-bVpBg",
        "measured": {
          "height": 449,
          "width": 320
        },
        "position": {
          "x": 106.99999999999991,
          "y": 247
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "Prompt Template-qHrW7",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {
              "template": []
            },
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt Template",
            "documentation": "https://docs.langflow.org/components-prompts",
            "edited": false,
            "error": null,
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "frozen": false,
            "full_path": null,
            "icon": "braces",
            "is_composition": null,
            "is_input": null,
            "is_output": null,
            "legacy": false,
            "lf_version": "1.7.2",
            "metadata": {
              "code_hash": "7382d03ce412",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  }
                ],
                "total_dependencies": 1
              },
              "module": "lfx.components.models_and_agents.prompt.PromptComponent"
            },
            "minimized": false,
            "name": "",
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Prompt",
                "group_outputs": false,
                "hidden": null,
                "loop_types": null,
                "method": "build_prompt",
                "name": "prompt",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "priority": 0,
            "replacement": null,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from lfx.base.prompts.api_utils import process_prompt_template\nfrom lfx.custom.custom_component.component import Component\nfrom lfx.inputs.inputs import DefaultPromptField\nfrom lfx.io import MessageTextInput, Output, PromptInput\nfrom lfx.schema.message import Message\nfrom lfx.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt Template\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    documentation: str = \"https://docs.langflow.org/components-prompts\"\n    icon = \"braces\"\n    trace_type = \"prompt\"\n    name = \"Prompt Template\"\n    priority = 0  # Set priority to 0 to make it appear first\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
              },
              "template": {
                "_input_type": "PromptInput",
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "template",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "track_in_telemetry": false,
                "type": "prompt",
                "value": "---\n\nYou are a Senior Knowledge Graph Agent with advanced reasoning capabilities. You have access to two complementary tools that work together to answer questions about a knowledge base containing entities, relationships, and semantic content.\n\n## Available Tools\n\n### 1. semantic_search\n**Purpose:** Find information based on meaning, context, or topics\n**Best for:**\n- Conceptual questions (\"What is the goal of...\", \"Find projects about...\")\n- When you don't know exact entity names\n- Topic-based queries (\"research on autonomous systems\")\n- Content analysis (\"summarize the document about...\")\n- Ambiguous queries that need semantic understanding\n\n**Input:** Natural language query or description\n**Returns:** Relevant entities with their content, ranked by semantic + keyword relevance\n\n### 2. explore_knowledge_graph\n**Purpose:** Navigate relationships and organizational structure\n**Best for:**\n- Structural questions (\"Who leads...\", \"What is connected to...\")\n- Hierarchy exploration (\"What projects does X work on?\")\n- Relationship analysis (\"How is A related to B?\")\n- Direct connections (\"Who works for...\")\n- When you have exact entity names\n\n**Input:** Exact entity name (case-sensitive)\n**Returns:** Direct relationships, multi-hop paths, or graph analysis\n\n## Query Strategy Framework\n\n### Simple Queries (Single Tool)\n**Pattern:** Direct structural question with known entity\n```\nUser: \"Who leads Project Alpha?\"\nStrategy: \n1. explore_knowledge_graph(entity_name=\"Project Alpha\", query_type=\"relationships\")\n2. Look for LEADS relationship in results\n```\n\n**Pattern:** Content-based search\n```\nUser: \"Find research about neural networks\"\nStrategy:\n1. semantic_search(query=\"neural networks research\")\n2. Return top results with relevance scores\n```\n\n### Complex Queries (Multi-Tool)\n**Pattern:** Unknown entity name + structural query\n```\nUser: \"Who leads the autonomous agent project?\"\nStrategy:\n1. semantic_search(query=\"autonomous agent project\") → finds \"Project Alpha\"\n2. explore_knowledge_graph(entity_name=\"Project Alpha\") → finds leader\n3. Synthesize: \"Dr. Sarah Chen leads Project Alpha (the autonomous agent project)\"\n```\n\n**Pattern:** Relationship discovery with context\n```\nUser: \"What projects is the Chief AI Researcher working on?\"\nStrategy:\n1. semantic_search(query=\"Chief AI Researcher\") → finds \"Dr. Sarah Chen\"\n2. explore_knowledge_graph(entity_name=\"Dr. Sarah Chen\", query_type=\"multi_hop\")\n3. Filter for PROJECT type entities in relationships\n```\n\n**Pattern:** Path finding\n```\nUser: \"How is Dr. Chen connected to the ML document?\"\nStrategy:\n1. explore_knowledge_graph(\n     entity_name=\"Dr. Sarah Chen\",\n     query_type=\"shortest_path\",\n     target_entity=\"Machine Learning Fundamentals\"\n   )\n2. Describe the path found\n```\n\n## Query Analysis Process\n\nBefore executing tools, analyze the query:\n\n1. **Identify Question Type:**\n   - Structural? → Start with explore_knowledge_graph\n   - Conceptual? → Start with semantic_search\n   - Complex? → Plan multi-step approach\n\n2. **Check for Entity Names:**\n   - Exact name known? → Use graph tool directly\n   - Fuzzy/description? → Use semantic search first\n\n3. **Determine Required Depth:**\n   - Direct relationship? → Basic graph query\n   - Multi-hop needed? → Use multi_hop query type\n   - Subgraph analysis? → Use subgraph query type\n\n## Response Guidelines\n\n### Always:\n- ✓ Base answers **only** on tool outputs\n- ✓ Cite which tool provided each piece of information\n- ✓ Acknowledge when information is not available\n- ✓ Suggest alternative queries if no results found\n- ✓ Combine results from multiple tools when needed\n\n### Never:\n- ✗ Hallucinate information not in tool outputs\n- ✗ Make assumptions about relationships not shown\n- ✗ Guess entity names if exact match not found\n- ✗ Ignore tool errors - surface them to the user\n\n## Example Interactions\n\n**Example 1: Direct Structural Query**\n```\nUser: \"What is Dr. Sarah Chen working on?\"\n\nReasoning: Known entity name, structural question\nAction: explore_knowledge_graph(entity_name=\"Dr. Sarah Chen\")\nResponse: \"Based on the knowledge graph, Dr. Sarah Chen:\n- LEADS Project Alpha (autonomous agent research)\n- WORKS_FOR AI Research Division\n- AUTHORED Machine Learning Fundamentals document\"\n```\n\n**Example 2: Semantic Discovery + Graph Navigation**\n```\nUser: \"Who is responsible for the AGI development project?\"\n\nReasoning: Unknown exact project name, need semantic search first\nAction 1: semantic_search(query=\"AGI development project\")\nResult: Finds \"Project Alpha\" (autonomous agent research for AGI)\n\nAction 2: explore_knowledge_graph(entity_name=\"Project Alpha\")\nResult: Dr. Sarah Chen LEADS Project Alpha\n\nResponse: \"The AGI development effort is Project Alpha, which is led by Dr. Sarah Chen. \nProject Alpha is focused on autonomous agent research toward AGI systems.\"\n```\n\n**Example 3: Complex Multi-Tool Analysis**\n```\nUser: \"Give me a summary of AI research activities and key people\"\n\nReasoning: Broad query needing both semantic content and structural info\n\nAction 1: semantic_search(query=\"AI research activities\")\nResult: Finds AI Research Division, Project Alpha, various papers\n\nAction 2: explore_knowledge_graph(entity_name=\"AI Research Division\", query_type=\"subgraph\")\nResult: Shows organizational structure and connections\n\nResponse: \"AI research activities summary:\n\nOrganization: AI Research Division (50+ researchers)\n- Focus: NLP, computer vision, reinforcement learning\n  \nActive Projects:\n- Project Alpha: AGI development via autonomous agents ($10M budget)\n  Led by: Dr. Sarah Chen\n\nKey Personnel:\n- Dr. Sarah Chen: Chief AI Researcher, 15 years experience\n  - Leads Project Alpha\n  - Authored ML Fundamentals guide\n  - Published 50+ papers\n\nThe semantic search found contextual information about research areas, while the knowledge \ngraph revealed the organizational structure and reporting relationships.\"\n```\n\n## Error Handling\n\nIf a tool returns an error or no results:\n\n1. **Acknowledge the limitation:**\n   \"I couldn't find information about [X] in the knowledge base.\"\n\n2. **Suggest alternatives:**\n   \"However, I can search for similar entities or try a different query approach.\"\n\n3. **Ask for clarification:**\n   \"Could you provide more details or try rephrasing the question?\"\n\n## Advanced Query Types\n\nFor power users, you can specify advanced queries:\n\n- **Importance Analysis:** \n  `explore_knowledge_graph(entity_name=\"X\", query_type=\"importance\")`\n  Returns centrality metrics and connection statistics\n\n- **Multi-Hop Traversal:**\n  `explore_knowledge_graph(entity_name=\"X\", query_type=\"multi_hop\")`\n  Explores indirect connections up to N hops away\n\n- **Path Finding:**\n  `explore_knowledge_graph(entity_name=\"A\", query_type=\"shortest_path\", target_entity=\"B\")`\n  Finds shortest connection path between entities\n\n## Context Awareness\n\nMaintain context across conversation turns:\n\n```\nUser: \"Tell me about Project Alpha\"\nAgent: [Uses semantic_search, provides summary]\n\nUser: \"Who leads it?\"\nAgent: [Uses explore_knowledge_graph on \"Project Alpha\" from context]\n```\n\n## Final Reminders\n\n- Be **precise** with entity names in graph queries\n- Use **semantic search** when uncertain about entity names\n- **Combine tools** for complex questions\n- Always **cite your sources** (which tool, what query)\n- **Explain your reasoning** when using multiple steps\n- Stay **grounded** in actual tool outputs - no speculation\n\nYou are a reliable, accurate assistant. When in doubt, ask for clarification rather than guessing.\n\n---"
              },
              "tool_placeholder": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Tool Placeholder",
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "tool_placeholder",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "Prompt Template"
        },
        "dragging": false,
        "id": "Prompt Template-qHrW7",
        "measured": {
          "height": 283,
          "width": 320
        },
        "position": {
          "x": 217,
          "y": 991
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "ChatOutput-S0ZIn",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "documentation": "https://docs.langflow.org/chat-input-and-output",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "context_id",
              "data_template",
              "clean_data"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "lf_version": "1.7.2",
            "metadata": {
              "code_hash": "8c87e536cca4",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "orjson",
                    "version": "3.10.15"
                  },
                  {
                    "name": "fastapi",
                    "version": "0.128.0"
                  },
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  }
                ],
                "total_dependencies": 3
              },
              "module": "lfx.components.input_output.chat_output.ChatOutput"
            },
            "minimized": true,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "clean_data": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Basic Clean Data",
                "dynamic": false,
                "info": "Whether to clean data before converting to string.",
                "list": false,
                "list_add_label": "Add More",
                "name": "clean_data",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": true
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from collections.abc import Generator\nfrom typing import Any\n\nimport orjson\nfrom fastapi.encoders import jsonable_encoder\n\nfrom lfx.base.io.chat import ChatComponent\nfrom lfx.helpers.data import safe_convert\nfrom lfx.inputs.inputs import BoolInput, DropdownInput, HandleInput, MessageTextInput\nfrom lfx.schema.data import Data\nfrom lfx.schema.dataframe import DataFrame\nfrom lfx.schema.message import Message\nfrom lfx.schema.properties import Source\nfrom lfx.template.field.base import Output\nfrom lfx.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    documentation: str = \"https://docs.langflow.org/chat-input-and-output\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized = True\n\n    inputs = [\n        HandleInput(\n            name=\"input_value\",\n            display_name=\"Inputs\",\n            info=\"Message to be passed as output.\",\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"context_id\",\n            display_name=\"Context ID\",\n            info=\"The context ID of the chat. Adds an extra layer to the local memory.\",\n            value=\"\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n        BoolInput(\n            name=\"clean_data\",\n            display_name=\"Basic Clean Data\",\n            value=True,\n            advanced=True,\n            info=\"Whether to clean data before converting to string.\",\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Output Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            # Handle case where source is a ChatOpenAI object\n            if hasattr(source, \"model_name\"):\n                source_dict[\"source\"] = source.model_name\n            elif hasattr(source, \"model\"):\n                source_dict[\"source\"] = str(source.model)\n            else:\n                source_dict[\"source\"] = str(source)\n        return Source(**source_dict)\n\n    async def message_response(self) -> Message:\n        # First convert the input to string if needed\n        text = self.convert_to_string()\n\n        # Get source properties\n        source, _, display_name, source_id = self.get_properties_from_source_component()\n\n        # Create or use existing Message object\n        if isinstance(self.input_value, Message) and not self.is_connected_to_chat_input():\n            message = self.input_value\n            # Update message properties\n            message.text = text\n            # Preserve existing session_id from the incoming message if it exists\n            existing_session_id = message.session_id\n        else:\n            message = Message(text=text)\n            existing_session_id = None\n\n        # Set message properties\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        # Preserve session_id from incoming message, or use component/graph session_id\n        message.session_id = (\n            self.session_id or existing_session_id or (self.graph.session_id if hasattr(self, \"graph\") else None) or \"\"\n        )\n        message.context_id = self.context_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(source_id, display_name, source)\n\n        # Store message if needed\n        if message.session_id and self.should_store_message:\n            stored_message = await self.send_message(message)\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n\n    def _serialize_data(self, data: Data) -> str:\n        \"\"\"Serialize Data object to JSON string.\"\"\"\n        # Convert data.data to JSON-serializable format\n        serializable_data = jsonable_encoder(data.data)\n        # Serialize with orjson, enabling pretty printing with indentation\n        json_bytes = orjson.dumps(serializable_data, option=orjson.OPT_INDENT_2)\n        # Convert bytes to string and wrap in Markdown code blocks\n        return \"```json\\n\" + json_bytes.decode(\"utf-8\") + \"\\n```\"\n\n    def _validate_input(self) -> None:\n        \"\"\"Validate the input data and raise ValueError if invalid.\"\"\"\n        if self.input_value is None:\n            msg = \"Input data cannot be None\"\n            raise ValueError(msg)\n        if isinstance(self.input_value, list) and not all(\n            isinstance(item, Message | Data | DataFrame | str) for item in self.input_value\n        ):\n            invalid_types = [\n                type(item).__name__\n                for item in self.input_value\n                if not isinstance(item, Message | Data | DataFrame | str)\n            ]\n            msg = f\"Expected Data or DataFrame or Message or str, got {invalid_types}\"\n            raise TypeError(msg)\n        if not isinstance(\n            self.input_value,\n            Message | Data | DataFrame | str | list | Generator | type(None),\n        ):\n            type_name = type(self.input_value).__name__\n            msg = f\"Expected Data or DataFrame or Message or str, Generator or None, got {type_name}\"\n            raise TypeError(msg)\n\n    def convert_to_string(self) -> str | Generator[Any, None, None]:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        self._validate_input()\n        if isinstance(self.input_value, list):\n            clean_data: bool = getattr(self, \"clean_data\", False)\n            return \"\\n\".join([safe_convert(item, clean_data=clean_data) for item in self.input_value])\n        if isinstance(self.input_value, Generator):\n            return self.input_value\n        return safe_convert(self.input_value)\n"
              },
              "context_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Context ID",
                "dynamic": false,
                "info": "The context ID of the chat. Adds an extra layer to the local memory.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "context_id",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "data_template": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Data Template",
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "data_template",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "{text}"
              },
              "input_value": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Inputs",
                "dynamic": false,
                "info": "Message to be passed as output.",
                "input_types": [
                  "Data",
                  "DataFrame",
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_value",
                "override_skip": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "other",
                "value": ""
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Sender Type",
                "dynamic": false,
                "external_options": {},
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "Machine"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sender_name",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "AI"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "session_id",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_store_message",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "showNode": false,
          "type": "ChatOutput"
        },
        "id": "ChatOutput-S0ZIn",
        "measured": {
          "height": 48,
          "width": 192
        },
        "position": {
          "x": 1397,
          "y": 114
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "Agent-s120G",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Define the agent's instructions, then enter a task to complete using tools.",
            "display_name": "Agent",
            "documentation": "https://docs.langflow.org/agents",
            "edited": false,
            "field_order": [
              "agent_llm",
              "api_key",
              "base_url",
              "project_id",
              "max_output_tokens",
              "max_tokens",
              "model_kwargs",
              "model_name",
              "openai_api_base",
              "api_key",
              "temperature",
              "seed",
              "max_retries",
              "timeout",
              "system_prompt",
              "context_id",
              "n_messages",
              "format_instructions",
              "output_schema",
              "tools",
              "input_value",
              "handle_parsing_errors",
              "verbose",
              "max_iterations",
              "agent_description",
              "add_current_date_tool"
            ],
            "frozen": false,
            "icon": "bot",
            "last_updated": "2026-01-12T02:44:36.563Z",
            "legacy": false,
            "lf_version": "1.7.2",
            "metadata": {
              "code_hash": "fba2d73636e5",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "langchain_core",
                    "version": "0.3.82"
                  },
                  {
                    "name": "pydantic",
                    "version": "2.11.10"
                  },
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  }
                ],
                "total_dependencies": 3
              },
              "module": "lfx.components.models_and_agents.agent.AgentComponent"
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Response",
                "group_outputs": false,
                "loop_types": null,
                "method": "message_response",
                "name": "response",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_frontend_node_flow_id": {
                "input_types": [],
                "value": "569a820f-2b87-4c05-84c8-085107a108d6"
              },
              "_frontend_node_folder_id": {
                "input_types": [],
                "value": "67c46809-3dbf-4d90-8f50-9b045df8edc1"
              },
              "_type": "Component",
              "add_current_date_tool": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Current Date",
                "dynamic": false,
                "info": "If true, will add a tool to the agent that returns the current date.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "add_current_date_tool",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": true
              },
              "agent_description": {
                "_input_type": "MultilineInput",
                "advanced": true,
                "ai_enabled": false,
                "copy_field": false,
                "display_name": "Agent Description [Deprecated]",
                "dynamic": false,
                "info": "The description of the agent. This is only used when in Tool Mode. Defaults to 'A helpful assistant with access to the following tools:' and tools are added dynamically. This feature is deprecated and will be removed in future versions.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "agent_description",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "A helpful assistant with access to the following tools:"
              },
              "agent_llm": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Language Model",
                "dynamic": false,
                "external_options": {
                  "fields": {
                    "data": {
                      "node": {
                        "display_name": "Connect other models",
                        "icon": "CornerDownLeft",
                        "name": "connect_other_models"
                      }
                    }
                  }
                },
                "info": "The provider of the language model that the agent will use to generate responses.",
                "input_types": [
                  "LanguageModel"
                ],
                "name": "agent_llm",
                "options": [
                  "Anthropic",
                  "Google Generative AI",
                  "OpenAI",
                  "IBM watsonx.ai",
                  "Ollama"
                ],
                "options_metadata": [
                  {
                    "icon": "Anthropic"
                  },
                  {
                    "icon": "GoogleGenerativeAI"
                  },
                  {
                    "icon": "OpenAI"
                  },
                  {
                    "icon": "WatsonxAI"
                  },
                  {
                    "icon": "Ollama"
                  }
                ],
                "override_skip": false,
                "placeholder": "Awaiting model input.",
                "real_time_refresh": true,
                "refresh_button": false,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": ""
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import json\nimport re\n\nfrom langchain_core.tools import StructuredTool, Tool\nfrom pydantic import ValidationError\n\nfrom lfx.base.agents.agent import LCToolsAgentComponent\nfrom lfx.base.agents.events import ExceptionWithMessageError\nfrom lfx.base.models.model_input_constants import (\n    ALL_PROVIDER_FIELDS,\n    MODEL_DYNAMIC_UPDATE_FIELDS,\n    MODEL_PROVIDERS_DICT,\n    MODEL_PROVIDERS_LIST,\n    MODELS_METADATA,\n)\nfrom lfx.base.models.model_utils import get_model_name\nfrom lfx.components.helpers import CurrentDateComponent\nfrom lfx.components.langchain_utilities.tool_calling import ToolCallingAgentComponent\nfrom lfx.components.models_and_agents.memory import MemoryComponent\nfrom lfx.custom.custom_component.component import get_component_toolkit\nfrom lfx.custom.utils import update_component_build_config\nfrom lfx.helpers.base_model import build_model_from_schema\nfrom lfx.inputs.inputs import BoolInput, SecretStrInput, StrInput\nfrom lfx.io import DropdownInput, IntInput, MessageTextInput, MultilineInput, Output, TableInput\nfrom lfx.log.logger import logger\nfrom lfx.schema.data import Data\nfrom lfx.schema.dotdict import dotdict\nfrom lfx.schema.message import Message\nfrom lfx.schema.table import EditMode\n\n\ndef set_advanced_true(component_input):\n    component_input.advanced = True\n    return component_input\n\n\nclass AgentComponent(ToolCallingAgentComponent):\n    display_name: str = \"Agent\"\n    description: str = \"Define the agent's instructions, then enter a task to complete using tools.\"\n    documentation: str = \"https://docs.langflow.org/agents\"\n    icon = \"bot\"\n    beta = False\n    name = \"Agent\"\n\n    memory_inputs = [set_advanced_true(component_input) for component_input in MemoryComponent().inputs]\n\n    # Filter out json_mode from OpenAI inputs since we handle structured output differently\n    if \"OpenAI\" in MODEL_PROVIDERS_DICT:\n        openai_inputs_filtered = [\n            input_field\n            for input_field in MODEL_PROVIDERS_DICT[\"OpenAI\"][\"inputs\"]\n            if not (hasattr(input_field, \"name\") and input_field.name == \"json_mode\")\n        ]\n    else:\n        openai_inputs_filtered = []\n\n    inputs = [\n        DropdownInput(\n            name=\"agent_llm\",\n            display_name=\"Model Provider\",\n            info=\"The provider of the language model that the agent will use to generate responses.\",\n            options=[*MODEL_PROVIDERS_LIST],\n            value=\"OpenAI\",\n            real_time_refresh=True,\n            refresh_button=False,\n            input_types=[],\n            options_metadata=[MODELS_METADATA[key] for key in MODEL_PROVIDERS_LIST if key in MODELS_METADATA],\n            external_options={\n                \"fields\": {\n                    \"data\": {\n                        \"node\": {\n                            \"name\": \"connect_other_models\",\n                            \"display_name\": \"Connect other models\",\n                            \"icon\": \"CornerDownLeft\",\n                        }\n                    }\n                },\n            },\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"API Key\",\n            info=\"The API key to use for the model.\",\n            required=True,\n        ),\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"The base URL of the API.\",\n            required=True,\n            show=False,\n        ),\n        StrInput(\n            name=\"project_id\",\n            display_name=\"Project ID\",\n            info=\"The project ID of the model.\",\n            required=True,\n            show=False,\n        ),\n        IntInput(\n            name=\"max_output_tokens\",\n            display_name=\"Max Output Tokens\",\n            info=\"The maximum number of tokens to generate.\",\n            show=False,\n        ),\n        *openai_inputs_filtered,\n        MultilineInput(\n            name=\"system_prompt\",\n            display_name=\"Agent Instructions\",\n            info=\"System Prompt: Initial instructions and context provided to guide the agent's behavior.\",\n            value=\"You are a helpful assistant that can use tools to answer questions and perform tasks.\",\n            advanced=False,\n        ),\n        MessageTextInput(\n            name=\"context_id\",\n            display_name=\"Context ID\",\n            info=\"The context ID of the chat. Adds an extra layer to the local memory.\",\n            value=\"\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"n_messages\",\n            display_name=\"Number of Chat History Messages\",\n            value=100,\n            info=\"Number of chat history messages to retrieve.\",\n            advanced=True,\n            show=True,\n        ),\n        MultilineInput(\n            name=\"format_instructions\",\n            display_name=\"Output Format Instructions\",\n            info=\"Generic Template for structured output formatting. Valid only with Structured response.\",\n            value=(\n                \"You are an AI that extracts structured JSON objects from unstructured text. \"\n                \"Use a predefined schema with expected types (str, int, float, bool, dict). \"\n                \"Extract ALL relevant instances that match the schema - if multiple patterns exist, capture them all. \"\n                \"Fill missing or ambiguous values with defaults: null for missing values. \"\n                \"Remove exact duplicates but keep variations that have different field values. \"\n                \"Always return valid JSON in the expected format, never throw errors. \"\n                \"If multiple objects can be extracted, return them all in the structured format.\"\n            ),\n            advanced=True,\n        ),\n        TableInput(\n            name=\"output_schema\",\n            display_name=\"Output Schema\",\n            info=(\n                \"Schema Validation: Define the structure and data types for structured output. \"\n                \"No validation if no output schema.\"\n            ),\n            advanced=True,\n            required=False,\n            value=[],\n            table_schema=[\n                {\n                    \"name\": \"name\",\n                    \"display_name\": \"Name\",\n                    \"type\": \"str\",\n                    \"description\": \"Specify the name of the output field.\",\n                    \"default\": \"field\",\n                    \"edit_mode\": EditMode.INLINE,\n                },\n                {\n                    \"name\": \"description\",\n                    \"display_name\": \"Description\",\n                    \"type\": \"str\",\n                    \"description\": \"Describe the purpose of the output field.\",\n                    \"default\": \"description of field\",\n                    \"edit_mode\": EditMode.POPOVER,\n                },\n                {\n                    \"name\": \"type\",\n                    \"display_name\": \"Type\",\n                    \"type\": \"str\",\n                    \"edit_mode\": EditMode.INLINE,\n                    \"description\": (\"Indicate the data type of the output field (e.g., str, int, float, bool, dict).\"),\n                    \"options\": [\"str\", \"int\", \"float\", \"bool\", \"dict\"],\n                    \"default\": \"str\",\n                },\n                {\n                    \"name\": \"multiple\",\n                    \"display_name\": \"As List\",\n                    \"type\": \"boolean\",\n                    \"description\": \"Set to True if this output field should be a list of the specified type.\",\n                    \"default\": \"False\",\n                    \"edit_mode\": EditMode.INLINE,\n                },\n            ],\n        ),\n        *LCToolsAgentComponent.get_base_inputs(),\n        # removed memory inputs from agent component\n        # *memory_inputs,\n        BoolInput(\n            name=\"add_current_date_tool\",\n            display_name=\"Current Date\",\n            advanced=True,\n            info=\"If true, will add a tool to the agent that returns the current date.\",\n            value=True,\n        ),\n    ]\n    outputs = [\n        Output(name=\"response\", display_name=\"Response\", method=\"message_response\"),\n    ]\n\n    async def get_agent_requirements(self):\n        \"\"\"Get the agent requirements for the agent.\"\"\"\n        llm_model, display_name = await self.get_llm()\n        if llm_model is None:\n            msg = \"No language model selected. Please choose a model to proceed.\"\n            raise ValueError(msg)\n        self.model_name = get_model_name(llm_model, display_name=display_name)\n\n        # Get memory data\n        self.chat_history = await self.get_memory_data()\n        await logger.adebug(f\"Retrieved {len(self.chat_history)} chat history messages\")\n        if isinstance(self.chat_history, Message):\n            self.chat_history = [self.chat_history]\n\n        # Add current date tool if enabled\n        if self.add_current_date_tool:\n            if not isinstance(self.tools, list):  # type: ignore[has-type]\n                self.tools = []\n            current_date_tool = (await CurrentDateComponent(**self.get_base_args()).to_toolkit()).pop(0)\n\n            if not isinstance(current_date_tool, StructuredTool):\n                msg = \"CurrentDateComponent must be converted to a StructuredTool\"\n                raise TypeError(msg)\n            self.tools.append(current_date_tool)\n\n        # Set shared callbacks for tracing the tools used by the agent\n        self.set_tools_callbacks(self.tools, self._get_shared_callbacks())\n\n        return llm_model, self.chat_history, self.tools\n\n    async def message_response(self) -> Message:\n        try:\n            llm_model, self.chat_history, self.tools = await self.get_agent_requirements()\n            # Set up and run agent\n            self.set(\n                llm=llm_model,\n                tools=self.tools or [],\n                chat_history=self.chat_history,\n                input_value=self.input_value,\n                system_prompt=self.system_prompt,\n            )\n            agent = self.create_agent_runnable()\n            result = await self.run_agent(agent)\n\n            # Store result for potential JSON output\n            self._agent_result = result\n\n        except (ValueError, TypeError, KeyError) as e:\n            await logger.aerror(f\"{type(e).__name__}: {e!s}\")\n            raise\n        except ExceptionWithMessageError as e:\n            await logger.aerror(f\"ExceptionWithMessageError occurred: {e}\")\n            raise\n        # Avoid catching blind Exception; let truly unexpected exceptions propagate\n        except Exception as e:\n            await logger.aerror(f\"Unexpected error: {e!s}\")\n            raise\n        else:\n            return result\n\n    def _preprocess_schema(self, schema):\n        \"\"\"Preprocess schema to ensure correct data types for build_model_from_schema.\"\"\"\n        processed_schema = []\n        for field in schema:\n            processed_field = {\n                \"name\": str(field.get(\"name\", \"field\")),\n                \"type\": str(field.get(\"type\", \"str\")),\n                \"description\": str(field.get(\"description\", \"\")),\n                \"multiple\": field.get(\"multiple\", False),\n            }\n            # Ensure multiple is handled correctly\n            if isinstance(processed_field[\"multiple\"], str):\n                processed_field[\"multiple\"] = processed_field[\"multiple\"].lower() in [\n                    \"true\",\n                    \"1\",\n                    \"t\",\n                    \"y\",\n                    \"yes\",\n                ]\n            processed_schema.append(processed_field)\n        return processed_schema\n\n    async def build_structured_output_base(self, content: str):\n        \"\"\"Build structured output with optional BaseModel validation.\"\"\"\n        json_pattern = r\"\\{.*\\}\"\n        schema_error_msg = \"Try setting an output schema\"\n\n        # Try to parse content as JSON first\n        json_data = None\n        try:\n            json_data = json.loads(content)\n        except json.JSONDecodeError:\n            json_match = re.search(json_pattern, content, re.DOTALL)\n            if json_match:\n                try:\n                    json_data = json.loads(json_match.group())\n                except json.JSONDecodeError:\n                    return {\"content\": content, \"error\": schema_error_msg}\n            else:\n                return {\"content\": content, \"error\": schema_error_msg}\n\n        # If no output schema provided, return parsed JSON without validation\n        if not hasattr(self, \"output_schema\") or not self.output_schema or len(self.output_schema) == 0:\n            return json_data\n\n        # Use BaseModel validation with schema\n        try:\n            processed_schema = self._preprocess_schema(self.output_schema)\n            output_model = build_model_from_schema(processed_schema)\n\n            # Validate against the schema\n            if isinstance(json_data, list):\n                # Multiple objects\n                validated_objects = []\n                for item in json_data:\n                    try:\n                        validated_obj = output_model.model_validate(item)\n                        validated_objects.append(validated_obj.model_dump())\n                    except ValidationError as e:\n                        await logger.aerror(f\"Validation error for item: {e}\")\n                        # Include invalid items with error info\n                        validated_objects.append({\"data\": item, \"validation_error\": str(e)})\n                return validated_objects\n\n            # Single object\n            try:\n                validated_obj = output_model.model_validate(json_data)\n                return [validated_obj.model_dump()]  # Return as list for consistency\n            except ValidationError as e:\n                await logger.aerror(f\"Validation error: {e}\")\n                return [{\"data\": json_data, \"validation_error\": str(e)}]\n\n        except (TypeError, ValueError) as e:\n            await logger.aerror(f\"Error building structured output: {e}\")\n            # Fallback to parsed JSON without validation\n            return json_data\n\n    async def json_response(self) -> Data:\n        \"\"\"Convert agent response to structured JSON Data output with schema validation.\"\"\"\n        # Always use structured chat agent for JSON response mode for better JSON formatting\n        try:\n            system_components = []\n\n            # 1. Agent Instructions (system_prompt)\n            agent_instructions = getattr(self, \"system_prompt\", \"\") or \"\"\n            if agent_instructions:\n                system_components.append(f\"{agent_instructions}\")\n\n            # 2. Format Instructions\n            format_instructions = getattr(self, \"format_instructions\", \"\") or \"\"\n            if format_instructions:\n                system_components.append(f\"Format instructions: {format_instructions}\")\n\n            # 3. Schema Information from BaseModel\n            if hasattr(self, \"output_schema\") and self.output_schema and len(self.output_schema) > 0:\n                try:\n                    processed_schema = self._preprocess_schema(self.output_schema)\n                    output_model = build_model_from_schema(processed_schema)\n                    schema_dict = output_model.model_json_schema()\n                    schema_info = (\n                        \"You are given some text that may include format instructions, \"\n                        \"explanations, or other content alongside a JSON schema.\\n\\n\"\n                        \"Your task:\\n\"\n                        \"- Extract only the JSON schema.\\n\"\n                        \"- Return it as valid JSON.\\n\"\n                        \"- Do not include format instructions, explanations, or extra text.\\n\\n\"\n                        \"Input:\\n\"\n                        f\"{json.dumps(schema_dict, indent=2)}\\n\\n\"\n                        \"Output (only JSON schema):\"\n                    )\n                    system_components.append(schema_info)\n                except (ValidationError, ValueError, TypeError, KeyError) as e:\n                    await logger.aerror(f\"Could not build schema for prompt: {e}\", exc_info=True)\n\n            # Combine all components\n            combined_instructions = \"\\n\\n\".join(system_components) if system_components else \"\"\n            llm_model, self.chat_history, self.tools = await self.get_agent_requirements()\n            self.set(\n                llm=llm_model,\n                tools=self.tools or [],\n                chat_history=self.chat_history,\n                input_value=self.input_value,\n                system_prompt=combined_instructions,\n            )\n\n            # Create and run structured chat agent\n            try:\n                structured_agent = self.create_agent_runnable()\n            except (NotImplementedError, ValueError, TypeError) as e:\n                await logger.aerror(f\"Error with structured chat agent: {e}\")\n                raise\n            try:\n                result = await self.run_agent(structured_agent)\n            except (\n                ExceptionWithMessageError,\n                ValueError,\n                TypeError,\n                RuntimeError,\n            ) as e:\n                await logger.aerror(f\"Error with structured agent result: {e}\")\n                raise\n            # Extract content from structured agent result\n            if hasattr(result, \"content\"):\n                content = result.content\n            elif hasattr(result, \"text\"):\n                content = result.text\n            else:\n                content = str(result)\n\n        except (\n            ExceptionWithMessageError,\n            ValueError,\n            TypeError,\n            NotImplementedError,\n            AttributeError,\n        ) as e:\n            await logger.aerror(f\"Error with structured chat agent: {e}\")\n            # Fallback to regular agent\n            content_str = \"No content returned from agent\"\n            return Data(data={\"content\": content_str, \"error\": str(e)})\n\n        # Process with structured output validation\n        try:\n            structured_output = await self.build_structured_output_base(content)\n\n            # Handle different output formats\n            if isinstance(structured_output, list) and structured_output:\n                if len(structured_output) == 1:\n                    return Data(data=structured_output[0])\n                return Data(data={\"results\": structured_output})\n            if isinstance(structured_output, dict):\n                return Data(data=structured_output)\n            return Data(data={\"content\": content})\n\n        except (ValueError, TypeError) as e:\n            await logger.aerror(f\"Error in structured output processing: {e}\")\n            return Data(data={\"content\": content, \"error\": str(e)})\n\n    async def get_memory_data(self):\n        # TODO: This is a temporary fix to avoid message duplication. We should develop a function for this.\n        messages = (\n            await MemoryComponent(**self.get_base_args())\n            .set(\n                session_id=self.graph.session_id,\n                context_id=self.context_id,\n                order=\"Ascending\",\n                n_messages=self.n_messages,\n            )\n            .retrieve_messages()\n        )\n        return [\n            message for message in messages if getattr(message, \"id\", None) != getattr(self.input_value, \"id\", None)\n        ]\n\n    async def get_llm(self):\n        if not isinstance(self.agent_llm, str):\n            return self.agent_llm, None\n\n        try:\n            provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n            if not provider_info:\n                msg = f\"Invalid model provider: {self.agent_llm}\"\n                raise ValueError(msg)\n\n            component_class = provider_info.get(\"component_class\")\n            display_name = component_class.display_name\n            inputs = provider_info.get(\"inputs\")\n            prefix = provider_info.get(\"prefix\", \"\")\n\n            return self._build_llm_model(component_class, inputs, prefix), display_name\n\n        except (AttributeError, ValueError, TypeError, RuntimeError) as e:\n            await logger.aerror(f\"Error building {self.agent_llm} language model: {e!s}\")\n            msg = f\"Failed to initialize language model: {e!s}\"\n            raise ValueError(msg) from e\n\n    def _build_llm_model(self, component, inputs, prefix=\"\"):\n        model_kwargs = {}\n        for input_ in inputs:\n            if hasattr(self, f\"{prefix}{input_.name}\"):\n                model_kwargs[input_.name] = getattr(self, f\"{prefix}{input_.name}\")\n        return component.set(**model_kwargs).build_model()\n\n    def set_component_params(self, component):\n        provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n        if provider_info:\n            inputs = provider_info.get(\"inputs\")\n            prefix = provider_info.get(\"prefix\")\n            # Filter out json_mode and only use attributes that exist on this component\n            model_kwargs = {}\n            for input_ in inputs:\n                if hasattr(self, f\"{prefix}{input_.name}\"):\n                    model_kwargs[input_.name] = getattr(self, f\"{prefix}{input_.name}\")\n\n            return component.set(**model_kwargs)\n        return component\n\n    def delete_fields(self, build_config: dotdict, fields: dict | list[str]) -> None:\n        \"\"\"Delete specified fields from build_config.\"\"\"\n        for field in fields:\n            if build_config is not None and field in build_config:\n                build_config.pop(field, None)\n\n    def update_input_types(self, build_config: dotdict) -> dotdict:\n        \"\"\"Update input types for all fields in build_config.\"\"\"\n        for key, value in build_config.items():\n            if isinstance(value, dict):\n                if value.get(\"input_types\") is None:\n                    build_config[key][\"input_types\"] = []\n            elif hasattr(value, \"input_types\") and value.input_types is None:\n                value.input_types = []\n        return build_config\n\n    async def update_build_config(\n        self, build_config: dotdict, field_value: str, field_name: str | None = None\n    ) -> dotdict:\n        # Iterate over all providers in the MODEL_PROVIDERS_DICT\n        # Existing logic for updating build_config\n        if field_name in (\"agent_llm\",):\n            build_config[\"agent_llm\"][\"value\"] = field_value\n            provider_info = MODEL_PROVIDERS_DICT.get(field_value)\n            if provider_info:\n                component_class = provider_info.get(\"component_class\")\n                if component_class and hasattr(component_class, \"update_build_config\"):\n                    # Call the component class's update_build_config method\n                    build_config = await update_component_build_config(\n                        component_class, build_config, field_value, \"model_name\"\n                    )\n\n            provider_configs: dict[str, tuple[dict, list[dict]]] = {\n                provider: (\n                    MODEL_PROVIDERS_DICT[provider][\"fields\"],\n                    [\n                        MODEL_PROVIDERS_DICT[other_provider][\"fields\"]\n                        for other_provider in MODEL_PROVIDERS_DICT\n                        if other_provider != provider\n                    ],\n                )\n                for provider in MODEL_PROVIDERS_DICT\n            }\n            if field_value in provider_configs:\n                fields_to_add, fields_to_delete = provider_configs[field_value]\n\n                # Delete fields from other providers\n                for fields in fields_to_delete:\n                    self.delete_fields(build_config, fields)\n\n                # Add provider-specific fields\n                if field_value == \"OpenAI\" and not any(field in build_config for field in fields_to_add):\n                    build_config.update(fields_to_add)\n                else:\n                    build_config.update(fields_to_add)\n                # Reset input types for agent_llm\n                build_config[\"agent_llm\"][\"input_types\"] = []\n                build_config[\"agent_llm\"][\"display_name\"] = \"Model Provider\"\n            elif field_value == \"connect_other_models\":\n                # Delete all provider fields\n                self.delete_fields(build_config, ALL_PROVIDER_FIELDS)\n                # # Update with custom component\n                custom_component = DropdownInput(\n                    name=\"agent_llm\",\n                    display_name=\"Language Model\",\n                    info=\"The provider of the language model that the agent will use to generate responses.\",\n                    options=[*MODEL_PROVIDERS_LIST],\n                    real_time_refresh=True,\n                    refresh_button=False,\n                    input_types=[\"LanguageModel\"],\n                    placeholder=\"Awaiting model input.\",\n                    options_metadata=[MODELS_METADATA[key] for key in MODEL_PROVIDERS_LIST if key in MODELS_METADATA],\n                    external_options={\n                        \"fields\": {\n                            \"data\": {\n                                \"node\": {\n                                    \"name\": \"connect_other_models\",\n                                    \"display_name\": \"Connect other models\",\n                                    \"icon\": \"CornerDownLeft\",\n                                },\n                            }\n                        },\n                    },\n                )\n                build_config.update({\"agent_llm\": custom_component.to_dict()})\n            # Update input types for all fields\n            build_config = self.update_input_types(build_config)\n\n            # Validate required keys\n            default_keys = [\n                \"code\",\n                \"_type\",\n                \"agent_llm\",\n                \"tools\",\n                \"input_value\",\n                \"add_current_date_tool\",\n                \"system_prompt\",\n                \"agent_description\",\n                \"max_iterations\",\n                \"handle_parsing_errors\",\n                \"verbose\",\n            ]\n            missing_keys = [key for key in default_keys if key not in build_config]\n            if missing_keys:\n                msg = f\"Missing required keys in build_config: {missing_keys}\"\n                raise ValueError(msg)\n        if (\n            isinstance(self.agent_llm, str)\n            and self.agent_llm in MODEL_PROVIDERS_DICT\n            and field_name in MODEL_DYNAMIC_UPDATE_FIELDS\n        ):\n            provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n            if provider_info:\n                component_class = provider_info.get(\"component_class\")\n                component_class = self.set_component_params(component_class)\n                prefix = provider_info.get(\"prefix\")\n                if component_class and hasattr(component_class, \"update_build_config\"):\n                    # Call each component class's update_build_config method\n                    # remove the prefix from the field_name\n                    if isinstance(field_name, str) and isinstance(prefix, str):\n                        field_name_without_prefix = field_name.replace(prefix, \"\")\n                    else:\n                        field_name_without_prefix = field_name\n                    build_config = await update_component_build_config(\n                        component_class, build_config, field_value, field_name_without_prefix\n                    )\n        return dotdict({k: v.to_dict() if hasattr(v, \"to_dict\") else v for k, v in build_config.items()})\n\n    async def _get_tools(self) -> list[Tool]:\n        component_toolkit = get_component_toolkit()\n        tools_names = self._build_tools_names()\n        agent_description = self.get_tool_description()\n        # TODO: Agent Description Depreciated Feature to be removed\n        description = f\"{agent_description}{tools_names}\"\n\n        tools = component_toolkit(component=self).get_tools(\n            tool_name=\"Call_Agent\",\n            tool_description=description,\n            # here we do not use the shared callbacks as we are exposing the agent as a tool\n            callbacks=self.get_langchain_callbacks(),\n        )\n        if hasattr(self, \"tools_metadata\"):\n            tools = component_toolkit(component=self, metadata=self.tools_metadata).update_tools_metadata(tools=tools)\n\n        return tools\n"
              },
              "context_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Context ID",
                "dynamic": false,
                "info": "The context ID of the chat. Adds an extra layer to the local memory.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "context_id",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "format_instructions": {
                "_input_type": "MultilineInput",
                "advanced": true,
                "ai_enabled": false,
                "copy_field": false,
                "display_name": "Output Format Instructions",
                "dynamic": false,
                "info": "Generic Template for structured output formatting. Valid only with Structured response.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "format_instructions",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "You are an AI that extracts structured JSON objects from unstructured text. Use a predefined schema with expected types (str, int, float, bool, dict). Extract ALL relevant instances that match the schema - if multiple patterns exist, capture them all. Fill missing or ambiguous values with defaults: null for missing values. Remove exact duplicates but keep variations that have different field values. Always return valid JSON in the expected format, never throw errors. If multiple objects can be extracted, return them all in the structured format."
              },
              "handle_parsing_errors": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Handle Parse Errors",
                "dynamic": false,
                "info": "Should the Agent fix errors when reading user input for better processing?",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "handle_parsing_errors",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": true
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "The input provided by the user for the agent to process.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "is_refresh": false,
              "max_iterations": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Iterations",
                "dynamic": false,
                "info": "The maximum number of attempts the agent can make to complete its task before it stops.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "max_iterations",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 15
              },
              "n_messages": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Number of Chat History Messages",
                "dynamic": false,
                "info": "Number of chat history messages to retrieve.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "n_messages",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 100
              },
              "output_schema": {
                "_input_type": "TableInput",
                "advanced": true,
                "display_name": "Output Schema",
                "dynamic": false,
                "info": "Schema Validation: Define the structure and data types for structured output. No validation if no output schema.",
                "input_types": [],
                "is_list": true,
                "list_add_label": "Add More",
                "name": "output_schema",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "table_icon": "Table",
                "table_schema": [
                  {
                    "default": "field",
                    "description": "Specify the name of the output field.",
                    "display_name": "Name",
                    "edit_mode": "inline",
                    "name": "name",
                    "type": "str"
                  },
                  {
                    "default": "description of field",
                    "description": "Describe the purpose of the output field.",
                    "display_name": "Description",
                    "edit_mode": "popover",
                    "name": "description",
                    "type": "str"
                  },
                  {
                    "default": "str",
                    "description": "Indicate the data type of the output field (e.g., str, int, float, bool, dict).",
                    "display_name": "Type",
                    "edit_mode": "inline",
                    "name": "type",
                    "options": [
                      "str",
                      "int",
                      "float",
                      "bool",
                      "dict"
                    ],
                    "type": "str"
                  },
                  {
                    "default": "False",
                    "description": "Set to True if this output field should be a list of the specified type.",
                    "display_name": "As List",
                    "edit_mode": "inline",
                    "name": "multiple",
                    "type": "boolean"
                  }
                ],
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "trigger_icon": "Table",
                "trigger_text": "Open table",
                "type": "table",
                "value": []
              },
              "system_prompt": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "ai_enabled": false,
                "copy_field": false,
                "display_name": "Agent Instructions",
                "dynamic": false,
                "info": "System Prompt: Initial instructions and context provided to guide the agent's behavior.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_prompt",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "You are a helpful assistant that can use tools to answer questions and perform tasks."
              },
              "tools": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Tools",
                "dynamic": false,
                "info": "These are the tools that the agent can use to help with tasks.",
                "input_types": [
                  "Tool"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "tools",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "other",
                "value": ""
              },
              "verbose": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Verbose",
                "dynamic": false,
                "info": "",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "name": "verbose",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "Agent"
        },
        "dragging": false,
        "id": "Agent-s120G",
        "measured": {
          "height": 427,
          "width": 320
        },
        "position": {
          "x": 925,
          "y": 116
        },
        "selected": false,
        "type": "genericNode"
      }
    ],
    "viewport": {
      "x": 768.8038613646227,
      "y": 221.0321400033061,
      "zoom": 0.42997533285496986
    }
  },
  "description": "Empowering Language Engineering.",
  "endpoint_name": null,
  "id": "569a820f-2b87-4c05-84c8-085107a108d6",
  "is_component": false,
  "last_tested_version": "1.7.2",
  "name": "Agentic Hybrid-Graph RAG 2.0",
  "tags": []
}