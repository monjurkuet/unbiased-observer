# PHASE 8: AGENTIC RESEARCH - DETAILED IMPLEMENTATION PLAN

**Phase:** 8 - Agentic Research with Hypothesis Generation & Experiment Design
**Priority:** HIGH (Core Differentiator)
**Duration:** 4 months
**Team:** 2 AI Researchers + 1 Domain Expert
**Budget:** $150K

---

## EXECUTIVE SUMMARY

Transform the Autonomous Research Agent from a passive knowledge aggregator into an active scientific discovery partner. Implement multi-agent hypothesis generation, experiment design assistance, and autonomous research workflows based on 2025-2026 AI research trends.

**Success Criteria:**
- 70% of AI-generated hypotheses validated by domain experts
- 50% reduction in time from research question to experimental design
- 80% of generated hypotheses lead to funded research proposals

---

## ARCHITECTURE OVERVIEW

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HYPOTHESIS GENERATION ENGINE                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                MULTI-AGENT FRAMEWORK                       â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚ Literature     â”‚  â”‚ Data Analysis   â”‚  â”‚ Hypothesis  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ Agent          â”‚  â”‚ Agent           â”‚  â”‚ Agent       â”‚  â”‚   â”‚
â”‚  â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚             â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”˜   â”‚
â”‚  â”‚                        â”‚                    â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                           â”‚                    â”‚                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”‚                 EXPERIMENT DESIGN ASSISTANT                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Technology Stack:**
- **Multi-Agent Framework:** Custom agent orchestration with LangChain/LlamaIndex
- **LLM Integration:** GPT-4o + Claude-3.5-Sonnet for different reasoning tasks
- **Knowledge Integration:** Direct PostgreSQL + vector search
- **Validation Framework:** Statistical analysis + domain expert feedback loops
- **Experiment Design:** Statistical power analysis + methodology generation

---

## IMPLEMENTATION ROADMAP

### **Month 1: Multi-Agent Hypothesis Generation Foundation**

#### **Week 1-2: Agent Architecture & Communication**
**Tasks:**
- [ ] Design multi-agent communication protocol
- [ ] Implement agent orchestration framework
- [ ] Create agent memory and state management
- [ ] Setup inter-agent message passing system
- [ ] Implement agent specialization (literature, data, hypothesis)

**Deliverables:**
- [ ] Agent communication framework
- [ ] Basic agent classes with memory
- [ ] Message passing and coordination system
- [ ] Agent specialization framework

#### **Week 3-4: Literature Analysis Agent**
**Tasks:**
- [ ] Implement literature search and retrieval
- [ ] Create paper analysis and summarization
- [ ] Build citation network analysis
- [ ] Implement gap identification algorithms
- [ ] Add temporal trend analysis for research areas

**Deliverables:**
- [ ] LiteratureAgent class with full functionality
- [ ] Citation network analysis capabilities
- [ ] Research gap identification
- [ ] Trend analysis for research domains

### **Month 2: Data Analysis & Hypothesis Synthesis**

#### **Week 5-6: Data Analysis Agent**
**Tasks:**
- [ ] Implement knowledge graph pattern analysis
- [ ] Create connection strength algorithms
- [ ] Build anomaly detection for unexpected relationships
- [ ] Implement cluster analysis for research themes
- [ ] Add statistical analysis of entity relationships

**Deliverables:**
- [ ] DataAnalysisAgent with graph analytics
- [ ] Pattern recognition algorithms
- [ ] Statistical relationship analysis
- [ ] Anomaly detection system

#### **Week 7-8: Hypothesis Generation Agent**
**Tasks:**
- [ ] Implement hypothesis synthesis algorithms
- [ ] Create novelty assessment framework
- [ ] Build evidence grounding mechanisms
- [ ] Implement iterative refinement loops
- [ ] Add hypothesis validation scoring

**Deliverables:**
- [ ] HypothesisAgent with synthesis capabilities
- [ ] Novelty assessment algorithms
- [ ] Evidence grounding system
- [ ] Iterative refinement framework

### **Month 3: Experiment Design & Validation**

#### **Week 9-10: Experiment Design Assistant**
**Tasks:**
- [ ] Implement experimental methodology generation
- [ ] Create statistical power analysis
- [ ] Build protocol generation for different research types
- [ ] Implement cost-benefit analysis for experiments
- [ ] Add resource requirement estimation

**Deliverables:**
- [ ] ExperimentDesigner class
- [ ] Statistical analysis integration
- [ ] Protocol generation system
- [ ] Resource estimation algorithms

#### **Week 11-12: Validation & Feedback Loops**
**Tasks:**
- [ ] Implement hypothesis validation framework
- [ ] Create domain expert feedback integration
- [ ] Build iterative improvement algorithms
- [ ] Implement hypothesis tracking and versioning
- [ ] Add performance metrics and analytics

**Deliverables:**
- [ ] ValidationAgent for hypothesis assessment
- [ ] Expert feedback integration system
- [ ] Hypothesis versioning and tracking
- [ ] Performance analytics dashboard

### **Month 4: Integration & Production**

#### **Week 13-14: System Integration**
**Tasks:**
- [ ] Integrate all agents into cohesive system
- [ ] Implement end-to-end hypothesis generation workflows
- [ ] Create API endpoints for hypothesis generation
- [ ] Build experiment design interfaces
- [ ] Implement autonomous research workflows

**Deliverables:**
- [ ] Integrated multi-agent system
- [ ] API endpoints for external access
- [ ] Autonomous workflow orchestration
- [ ] System performance monitoring

#### **Week 15-16: Testing & Deployment**
**Tasks:**
- [ ] Comprehensive testing with domain experts
- [ ] Performance optimization and scaling
- [ ] Security hardening and access controls
- [ ] Production deployment and monitoring
- [ ] User training and documentation

**Deliverables:**
- [ ] Production-ready system
- [ ] Domain expert validation results
- [ ] Performance benchmarks
- [ ] User documentation and training materials

---

## TECHNICAL SPECIFICATIONS

### **Multi-Agent Framework**

#### **Agent Base Classes**
```python
class ResearchAgent:
    """Base class for all research agents"""

    def __init__(self, agent_id: str, llm_config: dict, memory_system: MemorySystem):
        self.agent_id = agent_id
        self.llm = self._initialize_llm(llm_config)
        self.memory = memory_system
        self.specialization = self._define_specialization()

    async def process_message(self, message: AgentMessage) -> AgentResponse:
        """Process incoming messages and generate responses"""
        pass

    async def collaborate(self, other_agents: List[ResearchAgent], task: ResearchTask) -> CollaborationResult:
        """Collaborate with other agents on research tasks"""
        pass

class AgentMessage:
    """Inter-agent communication protocol"""
    sender_id: str
    recipient_id: str
    message_type: MessageType
    content: dict
    timestamp: datetime
    context: ResearchContext

class ResearchContext:
    """Shared research context across agents"""
    research_question: str
    domain: str
    current_hypotheses: List[Hypothesis]
    evidence_base: EvidenceBase
    constraints: ResearchConstraints
```

#### **Specialized Agent Classes**
```python
class LiteratureAgent(ResearchAgent):
    """Agent specialized in literature analysis"""

    async def analyze_papers(self, papers: List[Paper]) -> LiteratureAnalysis:
        """Analyze collection of research papers"""
        pass

    async def identify_gaps(self, literature: LiteratureBase) -> ResearchGaps:
        """Identify gaps in current literature"""
        pass

    async def find_connections(self, concepts: List[str]) -> ConceptConnections:
        """Find connections between research concepts"""
        pass

class DataAnalysisAgent(ResearchAgent):
    """Agent specialized in knowledge graph analysis"""

    async def analyze_patterns(self, graph: KnowledgeGraph) -> PatternAnalysis:
        """Analyze patterns in knowledge graph"""
        pass

    async def detect_anomalies(self, relationships: List[Relationship]) -> Anomalies:
        """Detect anomalous relationships"""
        pass

    async def cluster_concepts(self, entities: List[Entity]) -> ConceptClusters:
        """Cluster related concepts"""
        pass

class HypothesisAgent(ResearchAgent):
    """Agent specialized in hypothesis generation"""

    async def generate_hypotheses(self, context: ResearchContext) -> List[Hypothesis]:
        """Generate novel hypotheses from context"""
        pass

    async def refine_hypothesis(self, hypothesis: Hypothesis, feedback: Feedback) -> Hypothesis:
        """Refine hypothesis based on feedback"""
        pass

    async def validate_hypothesis(self, hypothesis: Hypothesis) -> ValidationResult:
        """Validate hypothesis feasibility"""
        pass
```

### **Hypothesis Generation Pipeline**

#### **Hypothesis Data Structures**
```python
@dataclass
class Hypothesis:
    """Represents a research hypothesis"""
    id: str
    statement: str
    evidence: List[Evidence]
    confidence_score: float
    novelty_score: float
    feasibility_score: float
    generated_at: datetime
    agent_id: str
    research_context: ResearchContext

@dataclass
class Evidence:
    """Evidence supporting or refuting a hypothesis"""
    type: EvidenceType  # LITERATURE, DATA, EXPERT_OPINION
    source: str
    content: str
    strength: float  # 0.0 to 1.0
    timestamp: datetime

@dataclass
class ValidationResult:
    """Result of hypothesis validation"""
    hypothesis_id: str
    is_valid: bool
    validation_score: float
    issues: List[str]
    recommendations: List[str]
    validated_by: str  # agent or human
    validated_at: datetime
```

#### **Generation Algorithms**

##### **Literature + Data Synthesis**
```python
async def synthesize_hypothesis(
    self,
    literature_analysis: LiteratureAnalysis,
    data_patterns: PatternAnalysis,
    research_question: str
) -> Hypothesis:
    """Synthesize hypothesis from literature and data analysis"""

    # Step 1: Identify key concepts from literature
    key_concepts = self._extract_key_concepts(literature_analysis)

    # Step 2: Find data patterns related to concepts
    relevant_patterns = self._find_relevant_patterns(data_patterns, key_concepts)

    # Step 3: Generate hypothesis connecting literature gaps with data insights
    hypothesis_statement = await self._generate_statement(
        research_question, key_concepts, relevant_patterns
    )

    # Step 4: Gather evidence from both sources
    evidence = self._collect_evidence(literature_analysis, relevant_patterns)

    # Step 5: Calculate confidence scores
    confidence = self._calculate_confidence(evidence)

    return Hypothesis(
        statement=hypothesis_statement,
        evidence=evidence,
        confidence_score=confidence,
        novelty_score=self._assess_novelty(hypothesis_statement, literature_analysis),
        feasibility_score=self._assess_feasibility(hypothesis_statement)
    )
```

##### **Iterative Refinement**
```python
async def refine_hypothesis_iteratively(
    self,
    initial_hypothesis: Hypothesis,
    max_iterations: int = 5
) -> Hypothesis:
    """Iteratively refine hypothesis through agent collaboration"""

    current_hypothesis = initial_hypothesis

    for iteration in range(max_iterations):
        # Get feedback from other agents
        feedback = await self._gather_agent_feedback(current_hypothesis)

        # Check if refinement is needed
        if self._is_sufficient_quality(current_hypothesis, feedback):
            break

        # Refine hypothesis based on feedback
        current_hypothesis = await self._apply_refinements(
            current_hypothesis, feedback
        )

        # Update evidence and scores
        current_hypothesis = await self._update_evidence(current_hypothesis)

    return current_hypothesis
```

### **Experiment Design System**

#### **Experiment Design Classes**
```python
@dataclass
class ExperimentalDesign:
    """Complete experimental design specification"""
    hypothesis: Hypothesis
    methodology: Methodology
    statistical_analysis: StatisticalAnalysis
    resource_requirements: ResourceRequirements
    timeline: ExperimentTimeline
    risk_assessment: RiskAssessment

@dataclass
class Methodology:
    """Experimental methodology specification"""
    type: ExperimentType  # OBSERVATIONAL, INTERVENTIONAL, COMPUTATIONAL
    design: StudyDesign   # RCT, COHORT, CASE_CONTROL, etc.
    sample_size: SampleSizeCalculation
    variables: List[Variable]
    procedures: List[Procedure]
    controls: List[Control]

@dataclass
class StatisticalAnalysis:
    """Statistical analysis plan"""
    primary_outcome: OutcomeMeasure
    secondary_outcomes: List[OutcomeMeasure]
    power_analysis: PowerAnalysis
    statistical_tests: List[StatisticalTest]
    sample_size_justification: str
```

#### **Design Generation Algorithm**
```python
async def design_experiment(
    self,
    hypothesis: Hypothesis,
    constraints: ResearchConstraints
) -> ExperimentalDesign:
    """Generate complete experimental design for hypothesis"""

    # Step 1: Determine appropriate methodology
    methodology = await self._select_methodology(hypothesis)

    # Step 2: Calculate statistical power and sample size
    statistical_analysis = await self._design_statistics(hypothesis, methodology)

    # Step 3: Estimate resource requirements
    resources = await self._estimate_resources(methodology, statistical_analysis)

    # Step 4: Create timeline
    timeline = await self._create_timeline(methodology, resources)

    # Step 5: Assess risks
    risks = await self._assess_risks(methodology, constraints)

    return ExperimentalDesign(
        hypothesis=hypothesis,
        methodology=methodology,
        statistical_analysis=statistical_analysis,
        resource_requirements=resources,
        timeline=timeline,
        risk_assessment=risks
    )
```

---

## VALIDATION FRAMEWORK

### **Hypothesis Quality Metrics**

#### **Novelty Assessment**
```python
def assess_novelty(self, hypothesis: str, literature_base: LiteratureBase) -> float:
    """Assess novelty of hypothesis against existing literature"""

    # Semantic similarity to existing hypotheses
    existing_hypotheses = literature_base.get_all_hypotheses()
    similarities = [self._semantic_similarity(hypothesis, existing)
                   for existing in existing_hypotheses]

    # Novelty score (higher = more novel)
    novelty_score = 1.0 - max(similarities) if similarities else 1.0

    # Adjust for recency (recent similar work reduces novelty)
    recency_penalty = self._calculate_recency_penalty(hypothesis, literature_base)

    return novelty_score * (1.0 - recency_penalty)
```

#### **Evidence Strength**
```python
def calculate_evidence_strength(self, evidence_list: List[Evidence]) -> float:
    """Calculate overall strength of evidence supporting hypothesis"""

    if not evidence_list:
        return 0.0

    # Weight evidence by type and quality
    weighted_strengths = []
    for evidence in evidence_list:
        base_strength = evidence.strength
        type_multiplier = self._get_evidence_type_multiplier(evidence.type)
        quality_multiplier = self._assess_evidence_quality(evidence)

        weighted_strength = base_strength * type_multiplier * quality_multiplier
        weighted_strengths.append(weighted_strength)

    # Combine evidence (diminishing returns for additional evidence)
    combined_strength = sum(weighted_strengths) / (1 + sum(weighted_strengths))

    return combined_strength
```

### **Domain Expert Integration**

#### **Feedback Collection System**
```python
async def collect_expert_feedback(
    self,
    hypothesis: Hypothesis,
    experts: List[DomainExpert]
) -> FeedbackCollection:
    """Collect feedback from domain experts"""

    feedback_tasks = []
    for expert in experts:
        task = self._request_expert_feedback(hypothesis, expert)
        feedback_tasks.append(task)

    # Parallel feedback collection
    feedback_results = await asyncio.gather(*feedback_tasks)

    # Aggregate and analyze feedback
    aggregated_feedback = self._aggregate_feedback(feedback_results)

    return FeedbackCollection(
        hypothesis_id=hypothesis.id,
        expert_feedback=feedback_results,
        aggregated_feedback=aggregated_feedback,
        consensus_score=self._calculate_consensus(feedback_results)
    )
```

---

## INTEGRATION WITH EXISTING SYSTEM

### **API Endpoints**
```
POST /api/agentic/generate-hypothesis
{
  "research_question": "How does X affect Y?",
  "domain": "biology",
  "constraints": {...}
}

POST /api/agentic/design-experiment
{
  "hypothesis_id": "hyp_123",
  "constraints": {...}
}

GET  /api/agentic/hypotheses?status=validated
GET  /api/agentic/experiments?hypothesis_id=...
```

### **Database Extensions**
```sql
-- Hypothesis storage
CREATE TABLE hypotheses (
    id UUID PRIMARY KEY,
    statement TEXT NOT NULL,
    evidence JSONB,
    confidence_score FLOAT,
    novelty_score FLOAT,
    feasibility_score FLOAT,
    generated_at TIMESTAMP,
    agent_id VARCHAR(255),
    research_context JSONB,
    validation_status VARCHAR(50),
    validated_at TIMESTAMP
);

-- Experiment designs
CREATE TABLE experiments (
    id UUID PRIMARY KEY,
    hypothesis_id UUID REFERENCES hypotheses(id),
    methodology JSONB,
    statistical_analysis JSONB,
    resource_requirements JSONB,
    timeline JSONB,
    risk_assessment JSONB,
    created_at TIMESTAMP,
    status VARCHAR(50)
);

-- Agent interactions
CREATE TABLE agent_interactions (
    id UUID PRIMARY KEY,
    session_id UUID,
    agent_id VARCHAR(255),
    message_type VARCHAR(50),
    content JSONB,
    timestamp TIMESTAMP,
    response JSONB
);
```

### **Scheduler Integration**
```python
# Add to AgentScheduler
async def _hypothesis_generation_job(self):
    """Periodic hypothesis generation"""
    try:
        logger.info("Running hypothesis generation cycle...")

        # Generate hypotheses for active research questions
        hypotheses = await self.hypothesis_engine.generate_batch_hypotheses()

        # Queue validation tasks
        for hypothesis in hypotheses:
            await self.task_queue.add_task(
                task_type="VALIDATE_HYPOTHESIS",
                metadata={"hypothesis_id": hypothesis.id}
            )

        logger.info(f"Generated {len(hypotheses)} new hypotheses")

    except Exception as e:
        logger.error(f"Error in hypothesis generation: {e}", exc_info=True)
```

---

## TESTING & VALIDATION

### **Hypothesis Quality Testing**

#### **Automated Validation**
- **Semantic coherence:** Hypothesis statements are logically consistent
- **Evidence grounding:** All claims supported by cited evidence
- **Novelty assessment:** Comparison against existing literature
- **Feasibility analysis:** Technical and resource requirements

#### **Domain Expert Validation**
- **Panel review:** 3-5 domain experts evaluate each hypothesis
- **Scoring rubric:** Novelty, feasibility, impact, evidence quality
- **Iterative feedback:** Refine hypotheses based on expert input
- **Publication potential:** Assessment of journal submission viability

### **Experiment Design Validation**

#### **Statistical Review**
- **Power analysis:** Sample size calculations verified
- **Methodology appropriateness:** Study design matches hypothesis
- **Bias assessment:** Control for confounding variables
- **Ethical compliance:** Research ethics requirements met

#### **Resource Validation**
- **Cost estimation:** Budget requirements realistic
- **Timeline assessment:** Duration and milestones achievable
- **Equipment availability:** Required resources accessible
- **Personnel requirements:** Expertise and staffing needs

### **End-to-End Testing**

#### **Research Workflow Testing**
1. **Question formulation** â†’ Generate research question
2. **Literature review** â†’ Automated gap analysis
3. **Hypothesis generation** â†’ Multi-agent synthesis
4. **Experiment design** â†’ Complete protocol generation
5. **Validation cycle** â†’ Expert feedback integration
6. **Iteration** â†’ Refinement based on feedback

#### **Performance Benchmarks**
- **Generation time:** < 30 minutes per hypothesis
- **Validation accuracy:** > 80% expert agreement
- **Experiment feasibility:** > 70% designs deemed implementable
- **System reliability:** > 95% successful completions

---

## ETHICAL CONSIDERATIONS

### **Responsible AI Development**
- **Bias mitigation:** Regular audits for algorithmic bias
- **Transparency:** Clear explanation of AI-generated content
- **Human oversight:** Domain experts in decision-making loops
- **Data privacy:** Protection of sensitive research information

### **Research Ethics**
- **Authorship attribution:** Clear AI vs human contribution tracking
- **Plagiarism prevention:** Originality verification for generated content
- **Misinformation prevention:** Fact-checking and validation requirements
- **Dual-use concerns:** Assessment of research application ethics

---

## CONCLUSION

Phase 8 transforms the Autonomous Research Agent from a knowledge aggregator into a true scientific discovery partner. By implementing multi-agent hypothesis generation, experiment design assistance, and autonomous research workflows, the system will accelerate scientific discovery while maintaining rigorous validation standards.

**The result:** A comprehensive AI research assistant that can generate novel hypotheses, design validation experiments, and guide researchers through the entire scientific method - from question formulation to experimental execution.

**Ready for implementation?** ðŸš€